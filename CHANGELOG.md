# MeloFormer æ›´æ–°æ—¥å¿—

## v1.0.1 (2024-12-02) - ğŸ”¥ çƒ­ä¿®å¤

### ğŸ”§ å…³é”®ä¿®å¤

- **æå‰åˆ‡æ¢ num_workers**: Step 10 â†’ Step 10 (ç«‹å³åˆ‡æ¢)
  - é—®é¢˜: v1.0.0 åœ¨ Phase 2 ä»ç”¨ `num_workers=0`ï¼Œå¯¼è‡´ GPU åˆ©ç”¨ç‡ 70% æ—¶é—´ä¸º 0
  - è§£å†³: Step 10 æ—¶åˆ‡æ¢åˆ° `num_workers=8`ï¼Œä½†**ä¿æŒ `batch_size=1`**
  - æ•ˆæœ: GPU åˆ©ç”¨ç‡ç«‹å³æå‡ï¼Œç¼–è¯‘ç»§ç»­è¿›è¡Œï¼Œé¿å… OOM

### ğŸ“Š æ–°çš„ä¸‰é˜¶æ®µç­–ç•¥

| é˜¶æ®µ | Steps | batch_size | num_workers | ç›®çš„ |
|------|-------|-----------|-------------|------|
| **Phase 1** | 1-10 | 1 | 0 | é¿å…å¤šè¿›ç¨‹å†²çª |
| **Phase 2** | 11-100 | 1 | 8 | åŠ é€Ÿæ•°æ®åŠ è½½ + ç»§ç»­ç¼–è¯‘ |
| **Phase 3** | 101+ | 6 | 8 | å…¨é€Ÿè®­ç»ƒ |

### ğŸ’¡ æ ¸å¿ƒæ”¹è¿›

- âœ… **GPU åˆ©ç”¨ç‡æå‡**: Step 10 åç«‹å³æå‡åˆ° 80-90%
- âœ… **ç¼–è¯‘å®‰å…¨**: ä¿æŒ batch_size=1 ç›´åˆ° Step 100
- âœ… **æ•°æ®åŠ è½½åŠ é€Ÿ**: 8 workers é¢„åŠ è½½æ•°æ®ï¼ŒGPU ä¸å†ç­‰å¾…

---

## v1.0.0 (2024-12-02) - ğŸ‰ ç¨³å®šç‰ˆ

### âœ¨ é‡å¤§ç‰¹æ€§

- **ä¸‰é˜¶æ®µåŠ¨æ€ä¼˜åŒ–**: æ ¹æ®å®æµ‹æ•°æ®è‡ªåŠ¨è°ƒæ•´ batch_size å’Œ num_workers
  - **Phase 1** (Step 1-10): `batch_size=1`, `num_workers=0` (ç¼–è¯‘é˜¶æ®µï¼Œæœ€å°æ˜¾å­˜ 10GB)
  - **Phase 2** (Step 11-50): `batch_size=1`, `num_workers=0` (å†…å­˜ç¨³å®šæœŸ)
  - **Phase 3** (Step 51+): `batch_size=6`, `num_workers=8` (é«˜é€Ÿè®­ç»ƒï¼Œæ˜¾å­˜ 52GB)
  - æ€§èƒ½æå‡: ç›¸æ¯” v0.9.2 å¿« **50%** (batch 4â†’6)
  - å®‰å…¨æ€§: ç¼–è¯‘æ—¶æ˜¾å­˜ä»… 10GBï¼Œé¿å…æ‰€æœ‰ OOM é—®é¢˜

- **åŸºäºå®æµ‹æ•°æ®çš„é…ç½®**:
  - H800 80GB æµ‹è¯•éªŒè¯: batch_size=6 æ˜¾å­˜å³°å€¼ 52GBï¼Œç•™ 28GB ä½™é‡
  - å®Œå…¨æ¶ˆé™¤ç¼–è¯‘æ—¶ OOM é£é™©
  - è®­ç»ƒé˜¶æ®µæœ€å¤§åŒ–ååé‡

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

| ç‰ˆæœ¬ | batch_size | ç¼–è¯‘æ˜¾å­˜ | è®­ç»ƒæ˜¾å­˜ | ç›¸å¯¹é€Ÿåº¦ | ç¨³å®šæ€§ |
|------|-----------|---------|---------|---------|--------|
| v0.9.0 | 4 | ~40GB (OOMé£é™©) | ~40GB | 1.0x | âš ï¸ ä¸ç¨³å®š |
| v0.9.2 | 4 | ~40GB | ~40GB | 1.0x | âœ… ç¨³å®š |
| v1.0.0 | 6 | 10GB | 52GB | **1.5x** | âœ… éå¸¸ç¨³å®š |

### ğŸ¯ æ¨èé…ç½®

```bash
python train.py \
    --model_size small \
    --batch_size 6 \           # ç›®æ ‡ batch (Phase 3 è‡ªåŠ¨åˆ‡æ¢)
    --num_workers 8 \          # ç›®æ ‡ workers (Phase 3 è‡ªåŠ¨åˆ‡æ¢)
    --gradient_accumulation_steps 8 \
    --max_seq_len 8192
```

### ğŸ“ è¯´æ˜

- é€‚ç”¨äº H800 80GB / A100 80GB
- Step 1-50: ç¼–è¯‘å’Œç¨³å®šæœŸï¼Œæ˜¾å­˜å®‰å…¨ (~10GB)
- Step 51+: è‡ªåŠ¨åˆ‡æ¢åˆ° batch=6ï¼Œé€Ÿåº¦é£™å‡
- æ€»å…±çº¦ 25 åˆ†é’Ÿè¾¾åˆ°å…¨é€Ÿ (Step 50)

---

## v0.9.2 (2024-12-02)

### ğŸ”§ ç´§æ€¥ä¿®å¤

- **æ¸è¿›å¼ num_workers åˆ‡æ¢**: å»¶è¿Ÿåˆ‡æ¢æ—¶æœºé¿å…å†…å­˜å³°å€¼
  - Step 1-50: `num_workers=0`ï¼ˆç¼–è¯‘ + å†…å­˜ç¨³å®šæœŸï¼‰
  - Step 50+: `num_workers=8`ï¼ˆé«˜é€Ÿè®­ç»ƒï¼‰
  - æ ¹æœ¬åŸå› : Step 10 åˆ‡æ¢å¤ªæ—©ï¼Œ8 workers ç¬æ—¶åŠ è½½å¯¼è‡´ OOM
  - è§£å†³æ–¹æ¡ˆ: å»¶è¿Ÿåˆ° Step 50ï¼Œç»™å†…å­˜è¶³å¤Ÿç¼“å†²æ—¶é—´

- **åˆ†ç‰‡ç¼“å­˜ä¼˜åŒ–**: 6 â†’ 30 ä¸ªåˆ†ç‰‡
  - å†…å­˜å ç”¨: ~30-35GBï¼ˆæ•°æ®ï¼‰ + 40GBï¼ˆè®­ç»ƒï¼‰ = ~75GB
  - é¢„æœŸ Tokens/s: ~6000-7000ï¼ˆæ¯” v0.9.0 å¿« 6 å€ï¼‰

### ğŸ“ è¯´æ˜

- é€‚ç”¨äº 120GB å†…å­˜æœåŠ¡å™¨
- Step 1-50: æ•°æ®åŠ è½½æ…¢ä½†å†…å­˜å®‰å…¨
- Step 50+: åˆ‡æ¢åé€Ÿåº¦é£™å‡

---

## v0.9.1 (2024-12-02) - å·²åºŸå¼ƒ

âš ï¸ **æ­¤ç‰ˆæœ¬æœ‰ä¸¥é‡ OOM é—®é¢˜ï¼Œè¯·ä½¿ç”¨ v0.9.2**

### é—®é¢˜

- åˆ†ç‰‡ç¼“å­˜è®¾ç½®ä¸º 53 è¿‡äºæ¿€è¿›
- 8 ä¸ª worker å¹¶å‘åŠ è½½å¯¼è‡´å†…å­˜å³°å€¼è¶…é™

---

## v0.9.0 (2024-12-02)

### âœ¨ æ–°ç‰¹æ€§

- **åŠ¨æ€ num_workers åˆ‡æ¢**: è®­ç»ƒ 10 æ­¥åè‡ªåŠ¨ä» 0 åˆ‡æ¢åˆ° 8
  - Step 1-10: `num_workers=0` (ç¼–è¯‘é˜¶æ®µï¼Œé¿å…ç¬æ—¶å†…å­˜å³°å€¼è§¦å‘ OOM Killer)
  - Step 11+: `num_workers=8` (è®­ç»ƒé˜¶æ®µï¼Œæœ€å¤§åŒ–æ•°æ®åŠ è½½é€Ÿåº¦ï¼ŒGPU ç¨³å®š 100%)
  - æ— éœ€æ‰‹åŠ¨å¹²é¢„ï¼Œå®Œå…¨è‡ªåŠ¨åŒ–
  - è§£å†³äº† `num_workers=8` æ—¶ GPU åˆ©ç”¨ç‡ 0-100% è·³åŠ¨çš„é—®é¢˜

### ğŸ”§ ä¿®å¤

- **DataLoader OOM ä¿®å¤**: ç¼–è¯‘æ—¶å°† `num_workers` è®¾ä¸º 0ï¼Œé¿å…å†…å­˜æº¢å‡º
  - æ ¹æœ¬åŸå› : å¤šä¸ª worker è¿›ç¨‹ + ç¼–è¯‘æ—¶ç¬¦å·è®¡ç®—å¯¼è‡´å†…å­˜è¶…é™
  - å½±å“: è§£å†³ "DataLoader worker killed by signal: Killed" é”™è¯¯

- **Dynamo Cache é™åˆ¶æå‡**:
  - `cache_size_limit`: 512 â†’ 2048
  - `accumulated_cache_size_limit`: 1024 â†’ 4096
  - è§£å†³è®­ç»ƒ 150 æ­¥åçš„ `CacheLimitExceeded` å´©æºƒ

### ğŸ“ è¯´æ˜

- é¦–æ¬¡ç¼–è¯‘éœ€è¦ 10-20 åˆ†é’Ÿï¼Œå±äºæ­£å¸¸ç°è±¡
- Step 10 æ—¶ä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°å¿«é€Ÿ DataLoaderï¼ˆ8 workersï¼‰ï¼ŒGPU åˆ©ç”¨ç‡ä» 60-80% â†’ ç¨³å®š 100%
- æ¨èé…ç½®: H800 å•å¡ seq=8192, batch=4, gradient_accumulation=8, num_workers=8ï¼ˆé»˜è®¤ï¼‰

---

## v0.8.0 (2024-12-01)

### âœ¨ æ–°ç‰¹æ€§

- **FlexAttention + æ··åˆç²¾åº¦æ”¯æŒ**
  - å®ç° Summary Token + FlexAttention æœºåˆ¶
  - å¼ºåˆ¶ FP32 workaround è§£å†³ PyTorch 2.5 BF16 bug

- **é€‰æ‹©æ€§ Gradient Checkpointing**
  - Attention éƒ¨åˆ†: æ­£å¸¸æ‰§è¡Œ
  - FFN éƒ¨åˆ†: ä½¿ç”¨ checkpoint èŠ‚çœæ˜¾å­˜

### ğŸ¯ æ€§èƒ½

- æ¨¡å‹è§„æ ¼: small (17M), base (85M), large (200M), xlarge (450M)
- H800 å•å¡æé™: seq=8192, batch=4
- æ˜¾å­˜å ç”¨æ¯”çº¯ BF16 é«˜çº¦ 50% (FP32 workaround ä»£ä»·)

---

## v0.5.0 (2024-12-01)

### ğŸ‰ é¦–æ¬¡å‘å¸ƒ

- HID-based symbolic music generation
- Summary Token mechanism (SS, SR, RS, RR phases)
- Multi-GPU DDP training support
- Sharded data loading with LRU cache
