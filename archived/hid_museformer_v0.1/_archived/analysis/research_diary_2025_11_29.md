# 研究日记 - 2025年11月29日

## 主题：音符层面 Token 类型分析 & FC-Attention 机制深度理解

---

## 一、Token 类型相关性分析（NMI）

### 1.1 同小节内跨类型 NMI

| 组合 | NMI | 建议 |
|------|-----|------|
| T→P | 0.126 | 最高，保留 |
| T→V | 0.090 | 中等 |
| P→V | 0.088 | 中等 |
| T→D | 0.082 | 中等 |
| P→D | 0.082 | 中等 |
| D→V | 0.056 | **最低，可忽略** |

### 1.2 不同小节 (offset=1-4 平均) NMI

| 组合 | NMI | 相对 T→T | 建议 |
|------|-----|----------|------|
| **T→T** | **0.193** | 100% | 必看 |
| **P→P** | **0.144** | 75% | 必看 |
| **V→V** | **0.152** | 79% | 必看 |
| D→D | 0.093 | 48% | 可稀疏/不看 |
| T→P | 0.095 | 49% | 不确定 |
| P→T | 0.090 | 47% | 不确定 |
| T→D | 0.069 | 36% | 不看 |
| D→T | 0.063 | 33% | 不看 |
| T→V | 0.066 | 34% | 不看 |
| V→T | 0.062 | 32% | 不看 |
| P→D | 0.058 | 30% | 不看 |
| D→P | 0.056 | 29% | 不看 |
| P→V | 0.055 | 29% | 不看 |
| V→P | 0.052 | 27% | 不看 |
| D→V | 0.050 | 26% | 不看 |
| V→D | 0.048 | 25% | 不看 |

### 1.3 Token 可见性矩阵

**不同音符的 Token 可见性 (Query → Key)**

```
         Key:  T      P      D      V
Query T:      ✓✓     ?      ✗      ✗
      P:      ?      ✓✓     ✗      ✗
      D:      ✗      ✗      ✗      ✗
      V:      ✗      ✗      ✗      ✓✓

✓✓ = 必看 (NMI > 0.1)
?  = 不确定 (NMI ~0.09, 接近阈值)
✗  = 不看 (NMI < 0.07)
```

### 1.4 总结

| 类别 | 组合 | 结论 |
|------|------|------|
| 同音符内 | 全部 | 必须互看 |
| 必看 | T→T, P→P, V→V | NMI > 0.14 |
| 不看 | D→D, D→*, *→D, D↔V | NMI < 0.07 |
| 不确定 | T↔P | NMI ~0.09 (边界) |

---

## 二、Summary Token 机制详解

### 2.1 什么是 Summary Token

Summary Token（S）是 MuseFormer 中引入的特殊 token，每个小节（bar）后插入一个 S，用于压缩和传递该小节的信息。

```
原始序列: [Bar1 tokens] [Bar2 tokens] [Bar3 tokens] ...
加入 S 后: [Bar1 tokens] [S1] [Bar2 tokens] [S2] [Bar3 tokens] [S3] ...
```

### 2.2 四种注意力块

| 块 | 名称 | 含义 | 作用 |
|----|------|------|------|
| **ss** | Summary → Summary | S 之间互相看 | 粗粒度的跨 bar 交互 |
| **sr** | Summary ← Regular | S 聚合同 bar 内的 R | 信息压缩 |
| **rs** | Regular → Summary | R 读取 S 的信息 | 获取远距离上下文 |
| **rr** | Regular → Regular | R 之间互相看 | 细粒度的近距离交互 |

### 2.3 信息流向图

```
┌─────────────────────────────────────────────────────────────┐
│                     FC-Attention 信息流                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Bar1 的 R ────rr────→ Bar1/Bar2 的 R （细粒度，近距离）    │
│       │                                                     │
│       │ sr（聚合）                                           │
│       ↓                                                     │
│      S1 ────────ss────────→ S5 （粗粒度，S之间交互）         │
│                              │                              │
│                              │ rs（分发）                    │
│                              ↓                              │
│                         Bar5 的 R （获取远距离信息）          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 三、关键理解点

### 3.1 什么是可学习的 vs 固定的

| 部分 | 类型 | 说明 |
|------|------|------|
| Q、K、V 投影矩阵 | **可学习** | 训练时优化，决定"怎么看" |
| K2、V2 二次投影 | **可学习** | 训练时优化，让 S 更好地服务 R |
| 注意力权重 | **动态计算** | 每次输入不同，权重也不同 |
| Mask 模式 | **固定** | 设计时定好，决定"能不能看" |

### 3.2 sr → rs 的两层学习

**关键洞察：Summary Token 经历两层注意力学习**

```python
# 第一层 (sr): S 聚合 Bar 内的 R
Q_s = W_q @ S_embedding      # 可学习
K_r = W_k @ R_tokens         # 可学习
V_r = W_v @ R_tokens         # 可学习
scores = Q_s @ K_r.T / √d
weights = softmax(scores)    # 动态权重，不是固定平均！
sum_x2 = weights @ V_r       # S 学会总结 bar 信息

# 第二层 (rs): R 读取 S 的信息
K2_s = W_k2 @ sum_x2         # 二次投影，可学习
V2_s = W_v2 @ sum_x2         # 二次投影，可学习
# R 用自己的 Q 去查询 S 的 K2、V2
```

**为什么需要二次投影 K2、V2？**
- sr 中：S 是查询者（用 Q），R 是被查询者（提供 K、V）
- rs 中：R 是查询者，S 是被查询者
- 角色互换，需要不同的表示，所以重新投影

### 3.3 层数对比

| 路径 | 注意力层数 |
|------|-----------|
| R 直接看 R（rr） | 1 层 |
| R 通过 S 看远处 R（sr → rs） | 2 层 |
| R 通过 S 看更远的 R（sr → ss → rs） | 3 层 |

---

## 四、复杂度分析

### 4.1 计算复杂度对比

设：L = 总 token 数，B = bar 数，n = 每 bar token 数（L = B × n）

| 方案 | rr 部分 | sr + rs 部分 | 总复杂度 |
|------|---------|--------------|----------|
| 全连接 | O(L²) | 无 | **O(L²)** |
| 纯稀疏 | O(L × k) | 无 | O(L)，但丢失远距离 |
| **FC-Attention** | O(L × k) | O(L × B) | **O(L × B) ≈ O(L√L)** |

### 4.2 具体数字示例

假设：1000 个 token，50 个 bar，每 bar 20 个 token，窗口 k=60

| 方案 | 计算量 | 相对全连接 |
|------|--------|-----------|
| 全连接 | 1,000,000 | 100% |
| FC-Attention rr | 60,000 | 6% |
| FC-Attention sr | 1,000 | 0.1% |
| FC-Attention rs | 50,000 | 5% |
| **FC-Attention 总计** | **111,000** | **~11%** |

### 4.3 什么时候 FC-Attention 划算/不划算

**划算：** B 大、n 大、L 大（长序列）

**不划算：** B = 1（单 bar）、序列很短（几十个 token）

---

## 五、训练速度分析

| 序列长度 | 全连接 | FC-Attention |
|----------|--------|--------------|
| 256 token | 快 ✓ | 稍慢 |
| 1024 token | 慢 | 快 ✓ |
| 4096 token | OOM 💥 | 能跑 ✓ |

**FC-Attention 是为长序列设计的**

---

## 六、当前实现状态总结

### 6.1 我的 attention.py

**小节层面（已实现）：**
- 同乐器：全连接
- 跨乐器：offset ≤ 2 全连接 + offset = 4
- 只有 **rr 部分**，没有 Summary Token

**音符层面（待实现）：**
- 同音符内：全连接
- 不同音符：T→T, P→P, V→V 必看；D 相关移除

### 6.2 与 MuseFormer 原版对比

| 特性 | MuseFormer 原版 | 我的实现 |
|------|----------------|----------|
| Summary Token | ✅ 有 (ss, sr, rs, rr) | ❌ 没有 (只有 rr) |
| 位置编码 | 可学习 | **RoPE** |
| 乐器感知 | ❌ 不区分 | **✅ 区分同/跨乐器** |
| Token 类型感知 | ❌ 不区分 | **✅ 待实现** |

---

## 七、今日关键收获

1. **Token 类型差异明显**：T→T, P→P, V→V 必看；D 相关可忽略
2. **S 的总结是可学习的** - 通过注意力机制动态加权，不是简单平均
3. **微观层面的注意力规则是固定的** - mask 模式在设计时就定好
4. **sr → rs 是两层学习**，ss/rr 是一层学习
5. **FC-Attention 是空间换时间** - 用 S 的额外计算换取 rr 的大量省略

---

## 八、后续计划

- [ ] 实现 Token 类型级别的稀疏优化（D↔V 移除）
- [ ] 考虑是否添加 Summary Token
- [ ] 测试不同序列长度下的性能对比

---

## 九、参考代码位置

- MuseFormer 原始实现：`others/muzic/museformer/museformer/`
  - Summary Token 嵌入：`museformer_decoder.py` (lines 186-196)
  - 注意力计算：`attention/self_attention_v2s1/rpe_self_attention_v2s1.py`
  - Mask 生成：`attention_mask_generation/attention_mask_generation.py`
- 我的实现：`hid_museformer/model/attention.py`
