#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±å…¥è®­ç»ƒå‡†å¤‡æ£€æŸ¥è„šæœ¬

å¯¹è®­ç»ƒæµç¨‹è¿›è¡Œæ›´åŠ ç»†è‡´çš„æ£€æŸ¥ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
"""

import os
import sys
import time
import gc
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from collections import Counter, defaultdict
import traceback
import json
import random
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from data.tokenizer_v2 import HIDTokenizerV2, TokenInfo
from data.dataset import HIDMusicDataset, collate_fn
from model.attention import (
    FCAttentionMask, MultiHeadFCAttention, FCAttentionBlock,
    SummaryAttention, SummaryAttentionMask, SummaryAttentionBlock,
    SummaryTokenEmbedding, RotaryPositionEmbedding
)
from model.hid_museformer import HIDMuseFormer, create_model


@dataclass
class DeepCheckResult:
    """æ·±å…¥æ£€æŸ¥ç»“æœ"""
    name: str
    passed: bool
    message: str
    severity: str = "info"  # info, warning, error, critical
    details: Optional[Dict] = None


class DeepTrainingChecker:
    """æ·±å…¥è®­ç»ƒæ£€æŸ¥å™¨"""

    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.results: List[DeepCheckResult] = []
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def log(self, msg: str):
        if self.verbose:
            print(msg)

    def add_result(self, result: DeepCheckResult):
        self.results.append(result)
        icons = {"info": "â„¹", "warning": "âš ", "error": "âœ—", "critical": "ğŸ’€"}
        status = "âœ“" if result.passed else icons.get(result.severity, "âœ—")
        self.log(f"  [{status}] {result.name}: {result.message}")

    # ==================== 1. Tokenizer è¾¹ç•Œæ£€æŸ¥ ====================

    def check_tokenizer_edge_cases(self) -> List[DeepCheckResult]:
        """æ£€æŸ¥ Tokenizer è¾¹ç•Œæƒ…å†µ"""
        self.log("\n" + "=" * 60)
        self.log("1. TOKENIZER è¾¹ç•Œæƒ…å†µæ£€æŸ¥")
        self.log("=" * 60)

        results = []
        tokenizer = HIDTokenizerV2()

        # 1.1 ç©ºè¾“å…¥
        try:
            ids, infos = tokenizer.encode_with_info("", add_special=True)
            has_bos_eos = len(ids) >= 2 and ids[0] == tokenizer.bos_id and ids[-1] == tokenizer.eos_id
            results.append(DeepCheckResult(
                name="ç©ºè¾“å…¥å¤„ç†",
                passed=has_bos_eos,
                message=f"è¾“å‡º {len(ids)} tokens" + (" (BOS+EOS)" if has_bos_eos else " (ç¼ºå°‘ç‰¹æ®Štoken)"),
            ))
        except Exception as e:
            results.append(DeepCheckResult(
                name="ç©ºè¾“å…¥å¤„ç†",
                passed=False,
                message=f"å¼‚å¸¸: {str(e)}",
                severity="error",
            ))
        self.add_result(results[-1])

        # 1.2 æç«¯éŸ³é«˜å€¼
        extreme_pitches = [
            ("P0", "æœ€ä½éŸ³é«˜"),
            ("P127", "æœ€é«˜éŸ³é«˜"),
            ("P60", "ä¸­å¤®C"),
        ]
        for pitch, desc in extreme_pitches:
            test_content = f"#P0\nB0 T0 {pitch} L4 V16"
            ids, infos = tokenizer.encode_with_info(test_content)
            pitch_found = any(info.token_str == pitch for info in infos)
            results.append(DeepCheckResult(
                name=f"éŸ³é«˜è¾¹ç•Œ ({desc})",
                passed=pitch_found,
                message=f"{pitch} æ­£ç¡®ç¼–ç " if pitch_found else f"{pitch} ç¼–ç å¤±è´¥",
            ))
            self.add_result(results[-1])

        # 1.3 æç«¯æŒç»­æ—¶é—´
        extreme_durations = [("L1", "æœ€çŸ­"), ("L64", "æœ€é•¿"), ("L32", "ä¸­ç­‰")]
        for dur, desc in extreme_durations:
            test_content = f"#P0\nB0 T0 P60 {dur} V16"
            ids, infos = tokenizer.encode_with_info(test_content)
            dur_found = any(info.token_str == dur for info in infos)
            results.append(DeepCheckResult(
                name=f"æŒç»­æ—¶é—´è¾¹ç•Œ ({desc})",
                passed=dur_found,
                message=f"{dur} æ­£ç¡®ç¼–ç " if dur_found else f"{dur} ç¼–ç å¤±è´¥",
            ))
            self.add_result(results[-1])

        # 1.4 æç«¯åŠ›åº¦
        extreme_velocities = [("V0", "æœ€å¼±"), ("V31", "æœ€å¼º"), ("V16", "ä¸­ç­‰")]
        for vel, desc in extreme_velocities:
            test_content = f"#P0\nB0 T0 P60 L4 {vel}"
            ids, infos = tokenizer.encode_with_info(test_content)
            vel_found = any(info.token_str == vel for info in infos)
            results.append(DeepCheckResult(
                name=f"åŠ›åº¦è¾¹ç•Œ ({desc})",
                passed=vel_found,
                message=f"{vel} æ­£ç¡®ç¼–ç " if vel_found else f"{vel} ç¼–ç å¤±è´¥",
            ))
            self.add_result(results[-1])

        # 1.5 æ‰€æœ‰ä¹å™¨ token
        instrument_tests = [
            ("#D", "é¼“"),
            ("#P0", "é’¢ç´"),
            ("#P127", "éŸ³æ•ˆ"),
            ("#P24", "å‰ä»–"),
        ]
        for inst, desc in instrument_tests:
            test_content = f"{inst}\nB0 T0 P60 L4 V16"
            ids, infos = tokenizer.encode_with_info(test_content)
            inst_found = any(info.token_str == inst for info in infos)
            results.append(DeepCheckResult(
                name=f"ä¹å™¨ ({desc})",
                passed=inst_found,
                message=f"{inst} æ­£ç¡®ç¼–ç " if inst_found else f"{inst} ç¼–ç å¤±è´¥",
            ))
            self.add_result(results[-1])

        # 1.6 æ‰€æœ‰ä½ç½® token (T0-T15)
        position_errors = []
        for i in range(16):
            pos_token = f"T{i}"
            test_content = f"#P0\nB0 {pos_token} P60 L4 V16"
            ids, infos = tokenizer.encode_with_info(test_content)
            if not any(info.token_str == pos_token for info in infos):
                position_errors.append(pos_token)

        results.append(DeepCheckResult(
            name="ä½ç½® tokens (T0-T15)",
            passed=len(position_errors) == 0,
            message="å…¨éƒ¨ 16 ä¸ªæ­£ç¡®" if not position_errors else f"å¤±è´¥: {position_errors}",
        ))
        self.add_result(results[-1])

        # 1.7 token_type åˆ†é…ä¸€è‡´æ€§æ£€æŸ¥
        test_content = """#P0
B0 T0 P60 L4 V16
T4 P64 L4 V16
T8 P67 L4 V16
B1 T0 P72 L8 V20
"""
        ids, infos = tokenizer.encode_with_info(test_content)

        # æ£€æŸ¥æ¯ä¸ªéŸ³ç¬¦çš„ token_type åºåˆ—æ˜¯å¦åˆç† (T -> P -> L -> V)
        type_sequences = []
        current_seq = []
        for info in infos:
            if info.token_type == 0:  # T
                if current_seq:
                    type_sequences.append(current_seq)
                current_seq = [0]
            elif info.token_type in [1, 2, 3]:
                current_seq.append(info.token_type)

        if current_seq:
            type_sequences.append(current_seq)

        # æ£€æŸ¥æ¯ä¸ªåºåˆ—æ˜¯å¦ä¸º [0, 1, 2, 3] æˆ–å…¶å­åºåˆ—
        valid_sequences = all(
            seq == [0, 1, 2, 3] or seq == [0, 1] or seq == [0, 1, 2] or seq == [0, 1, 3]
            for seq in type_sequences
        )

        results.append(DeepCheckResult(
            name="Token ç±»å‹åºåˆ—ä¸€è‡´æ€§",
            passed=valid_sequences,
            message=f"æ£€æŸ¥ {len(type_sequences)} ä¸ªéŸ³ç¬¦åºåˆ—" + (" æ­£ç¡®" if valid_sequences else " æœ‰å¼‚å¸¸"),
            details={"sequences": type_sequences[:5]},
        ))
        self.add_result(results[-1])

        # 1.8 note_id è¿ç»­æ€§æ£€æŸ¥
        note_ids = [info.note_id for info in infos if info.note_id > 0]
        if note_ids:
            note_id_set = set(note_ids)
            expected_ids = set(range(1, max(note_ids) + 1))
            ids_match = note_id_set == expected_ids
            results.append(DeepCheckResult(
                name="Note ID è¿ç»­æ€§",
                passed=ids_match,
                message=f"ID èŒƒå›´ 1-{max(note_ids)}" + (" è¿ç»­" if ids_match else " ä¸è¿ç»­"),
            ))
            self.add_result(results[-1])

        # 1.9 å’Œå¼¦ token æ£€æŸ¥
        chord_content = """H 4 384 Q
T B0 T0 120
TS B0 T0 4/4
CHORDS 0:Cmaj 1:Amin 2:Fmaj 3:G7

#P0
B0 T0 P60 L4 V16
B1 T0 P57 L4 V16
B2 T0 P65 L4 V16
B3 T0 P55 L4 V16
"""
        ids, infos = tokenizer.encode_with_info(chord_content)
        chord_infos = [info for info in infos if info.is_chord]
        expected_chords = ['Cmaj', 'Amin', 'Fmaj', 'G7']
        found_chords = [info.token_str for info in chord_infos]

        chords_match = all(c in found_chords for c in expected_chords)
        results.append(DeepCheckResult(
            name="å’Œå¼¦ token è§£æ",
            passed=chords_match,
            message=f"æ‰¾åˆ° {len(chord_infos)} ä¸ªå’Œå¼¦: {found_chords[:4]}",
        ))
        self.add_result(results[-1])

        return results

    # ==================== 2. æ•°æ®åˆ†å¸ƒæ£€æŸ¥ ====================

    def check_data_distribution(self, data_path: Optional[str] = None) -> List[DeepCheckResult]:
        """æ£€æŸ¥æ•°æ®åˆ†å¸ƒ"""
        self.log("\n" + "=" * 60)
        self.log("2. æ•°æ®åˆ†å¸ƒæ£€æŸ¥")
        self.log("=" * 60)

        results = []
        tokenizer = HIDTokenizerV2()

        # ä½¿ç”¨æµ‹è¯•æ–‡ä»¶
        test_file = data_path or str(Path(__file__).parent / "data" / "test_output_with_chords.txt")

        if not os.path.exists(test_file):
            results.append(DeepCheckResult(
                name="æµ‹è¯•æ•°æ®æ–‡ä»¶",
                passed=False,
                message=f"æ‰¾ä¸åˆ°: {test_file}",
                severity="error",
            ))
            self.add_result(results[-1])
            return results

        # è¯»å–å¹¶åˆ†ææ•°æ®
        content = open(test_file).read()
        ids, infos = tokenizer.encode_with_info(content)

        # 2.1 åºåˆ—é•¿åº¦åˆ†æ
        seq_len = len(ids)
        results.append(DeepCheckResult(
            name="åºåˆ—é•¿åº¦",
            passed=seq_len > 0,
            message=f"{seq_len} tokens",
            details={"length": seq_len},
        ))
        self.add_result(results[-1])

        # 2.2 Token ç±»å‹åˆ†å¸ƒ
        type_counts = Counter()
        for info in infos:
            if info.token_type >= 0:
                type_names = {0: 'T', 1: 'P', 2: 'L', 3: 'V'}
                type_counts[type_names.get(info.token_type, 'Other')] += 1
            elif info.is_chord:
                type_counts['Chord'] += 1
            elif info.token_str in ['BOS', 'EOS', 'SEP']:
                type_counts['Special'] += 1
            elif info.token_str.startswith('#'):
                type_counts['Instrument'] += 1
            else:
                type_counts['Other'] += 1

        # æ£€æŸ¥ T, P, L, V æ•°é‡æ˜¯å¦æ¥è¿‘
        tplv_counts = [type_counts.get(t, 0) for t in ['T', 'P', 'L', 'V']]
        if all(c > 0 for c in tplv_counts):
            max_diff = max(tplv_counts) - min(tplv_counts)
            avg_count = sum(tplv_counts) / 4
            balance_ratio = max_diff / avg_count if avg_count > 0 else float('inf')
            is_balanced = balance_ratio < 0.5  # å·®å¼‚ä¸è¶…è¿‡å¹³å‡å€¼çš„ 50%
        else:
            is_balanced = False

        results.append(DeepCheckResult(
            name="Token ç±»å‹å¹³è¡¡",
            passed=is_balanced,
            message=f"T:{type_counts.get('T',0)} P:{type_counts.get('P',0)} L:{type_counts.get('L',0)} V:{type_counts.get('V',0)}",
            severity="warning" if not is_balanced else "info",
            details=dict(type_counts),
        ))
        self.add_result(results[-1])

        # 2.3 Bar åˆ†å¸ƒ
        bar_ids = [info.chord_idx for info in infos if info.chord_idx >= 0]
        if bar_ids:
            num_bars = max(bar_ids) + 1
            bar_counts = Counter(bar_ids)
            avg_tokens_per_bar = len(bar_ids) / num_bars

            results.append(DeepCheckResult(
                name="Bar åˆ†å¸ƒ",
                passed=True,
                message=f"{num_bars} bars, å¹³å‡ {avg_tokens_per_bar:.1f} tokens/bar",
                details={
                    "num_bars": num_bars,
                    "avg_tokens_per_bar": avg_tokens_per_bar,
                },
            ))
            self.add_result(results[-1])

        # 2.4 ä¹å™¨åˆ†å¸ƒ
        instrument_ids = [info.instrument_id for info in infos if info.instrument_id >= 0]
        if instrument_ids:
            unique_instruments = set(instrument_ids)
            inst_counts = Counter(instrument_ids)

            results.append(DeepCheckResult(
                name="ä¹å™¨åˆ†å¸ƒ",
                passed=len(unique_instruments) > 0,
                message=f"{len(unique_instruments)} ç§ä¹å™¨",
                details={"instruments": list(unique_instruments)[:10]},
            ))
            self.add_result(results[-1])

        # 2.5 éŸ³é«˜åˆ†å¸ƒ
        pitch_tokens = [info.token_str for info in infos if info.is_pitch]
        if pitch_tokens:
            pitches = [int(p[1:]) for p in pitch_tokens]
            pitch_range = max(pitches) - min(pitches)
            avg_pitch = sum(pitches) / len(pitches)

            results.append(DeepCheckResult(
                name="éŸ³é«˜åˆ†å¸ƒ",
                passed=True,
                message=f"èŒƒå›´ P{min(pitches)}-P{max(pitches)}, å¹³å‡ P{avg_pitch:.1f}",
                details={
                    "min": min(pitches),
                    "max": max(pitches),
                    "avg": avg_pitch,
                },
            ))
            self.add_result(results[-1])

        return results

    # ==================== 3. æ¢¯åº¦å’Œæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ ====================

    def check_gradient_stability(self) -> List[DeepCheckResult]:
        """æ£€æŸ¥æ¢¯åº¦å’Œæ•°å€¼ç¨³å®šæ€§"""
        self.log("\n" + "=" * 60)
        self.log("3. æ¢¯åº¦å’Œæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥")
        self.log("=" * 60)

        results = []
        vocab_size = 643

        # åˆ›å»ºæ¨¡å‹
        model = create_model(vocab_size=vocab_size, model_size='small')
        model.to(self.device)
        model.train()

        batch_size = 2
        seq_len = 256

        token_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(self.device)
        chord_ids = torch.randint(-1, 10, (batch_size, seq_len)).to(self.device)
        instrument_ids = torch.randint(0, 130, (batch_size, seq_len)).to(self.device)
        labels = token_ids.clone()

        # 3.1 å‰å‘ä¼ æ’­æ•°å€¼æ£€æŸ¥
        with torch.no_grad():
            logits = model(token_ids, chord_ids, instrument_ids)

            # æ£€æŸ¥ NaN
            has_nan = torch.isnan(logits).any()
            # æ£€æŸ¥ Inf
            has_inf = torch.isinf(logits).any()
            # æ£€æŸ¥æ•°å€¼èŒƒå›´
            logit_max = logits.abs().max().item()
            logit_mean = logits.abs().mean().item()

        results.append(DeepCheckResult(
            name="å‰å‘ä¼ æ’­ NaN æ£€æŸ¥",
            passed=not has_nan,
            message="æ—  NaN" if not has_nan else "å‘ç° NaN!",
            severity="critical" if has_nan else "info",
        ))
        self.add_result(results[-1])

        results.append(DeepCheckResult(
            name="å‰å‘ä¼ æ’­ Inf æ£€æŸ¥",
            passed=not has_inf,
            message="æ—  Inf" if not has_inf else "å‘ç° Inf!",
            severity="critical" if has_inf else "info",
        ))
        self.add_result(results[-1])

        results.append(DeepCheckResult(
            name="Logits æ•°å€¼èŒƒå›´",
            passed=logit_max < 100,
            message=f"max={logit_max:.2f}, mean={logit_mean:.4f}",
            severity="warning" if logit_max > 50 else "info",
        ))
        self.add_result(results[-1])

        # 3.2 æ¢¯åº¦æ£€æŸ¥
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        optimizer.zero_grad()

        logits = model(token_ids, chord_ids, instrument_ids)
        shift_logits = logits[:, :-1, :].contiguous()
        shift_labels = labels[:, 1:].contiguous()

        loss = F.cross_entropy(
            shift_logits.view(-1, vocab_size),
            shift_labels.view(-1),
            ignore_index=-100,
        )
        loss.backward()

        # æ£€æŸ¥æ¢¯åº¦
        grad_norms = []
        grad_nan_params = []
        grad_inf_params = []
        grad_zero_params = []

        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                grad_norms.append(grad_norm)

                if torch.isnan(param.grad).any():
                    grad_nan_params.append(name)
                if torch.isinf(param.grad).any():
                    grad_inf_params.append(name)
                if grad_norm == 0:
                    grad_zero_params.append(name)

        results.append(DeepCheckResult(
            name="æ¢¯åº¦ NaN æ£€æŸ¥",
            passed=len(grad_nan_params) == 0,
            message="æ—  NaN æ¢¯åº¦" if not grad_nan_params else f"å‘ç° {len(grad_nan_params)} ä¸ª",
            severity="critical" if grad_nan_params else "info",
        ))
        self.add_result(results[-1])

        results.append(DeepCheckResult(
            name="æ¢¯åº¦ Inf æ£€æŸ¥",
            passed=len(grad_inf_params) == 0,
            message="æ—  Inf æ¢¯åº¦" if not grad_inf_params else f"å‘ç° {len(grad_inf_params)} ä¸ª",
            severity="critical" if grad_inf_params else "info",
        ))
        self.add_result(results[-1])

        # æ¢¯åº¦èŒƒæ•°åˆ†å¸ƒ
        if grad_norms:
            avg_grad_norm = sum(grad_norms) / len(grad_norms)
            max_grad_norm = max(grad_norms)
            min_grad_norm = min(grad_norms)

            results.append(DeepCheckResult(
                name="æ¢¯åº¦èŒƒæ•°åˆ†å¸ƒ",
                passed=0 < avg_grad_norm < 10,
                message=f"avg={avg_grad_norm:.4f}, max={max_grad_norm:.4f}, min={min_grad_norm:.6f}",
                severity="warning" if avg_grad_norm > 5 or avg_grad_norm == 0 else "info",
            ))
            self.add_result(results[-1])

        # 3.3 æ¢¯åº¦è£å‰ªæ•ˆæœ
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        clipped_norms = []
        for param in model.parameters():
            if param.grad is not None:
                clipped_norms.append(param.grad.norm().item())

        if clipped_norms:
            max_clipped = max(clipped_norms)
            results.append(DeepCheckResult(
                name="æ¢¯åº¦è£å‰ªæ•ˆæœ",
                passed=max_clipped <= 1.1,  # å…è®¸å°‘é‡è¯¯å·®
                message=f"è£å‰ªå max={max_clipped:.4f}",
            ))
            self.add_result(results[-1])

        # 3.4 æƒé‡èŒƒæ•°æ£€æŸ¥
        weight_norms = []
        for name, param in model.named_parameters():
            if 'weight' in name:
                weight_norms.append((name, param.norm().item()))

        if weight_norms:
            avg_weight_norm = sum(n for _, n in weight_norms) / len(weight_norms)
            max_weight = max(weight_norms, key=lambda x: x[1])

            results.append(DeepCheckResult(
                name="æƒé‡èŒƒæ•°",
                passed=avg_weight_norm < 100,
                message=f"avg={avg_weight_norm:.2f}, max={max_weight[1]:.2f} ({max_weight[0][:30]}...)",
            ))
            self.add_result(results[-1])

        return results

    # ==================== 4. é•¿åºåˆ—å¤„ç†æ£€æŸ¥ ====================

    def check_long_sequence_handling(self) -> List[DeepCheckResult]:
        """æ£€æŸ¥é•¿åºåˆ—å¤„ç†"""
        self.log("\n" + "=" * 60)
        self.log("4. é•¿åºåˆ—å¤„ç†æ£€æŸ¥")
        self.log("=" * 60)

        results = []
        vocab_size = 643

        # 4.1 ä¸åŒåºåˆ—é•¿åº¦çš„å‰å‘ä¼ æ’­
        seq_lengths = [128, 512, 1024, 2048]

        for seq_len in seq_lengths:
            try:
                model = create_model(vocab_size=vocab_size, model_size='small', max_seq_len=max(seq_lengths) + 100)
                model.to(self.device)
                model.eval()

                token_ids = torch.randint(0, vocab_size, (1, seq_len)).to(self.device)
                chord_ids = torch.randint(-1, 10, (1, seq_len)).to(self.device)
                instrument_ids = torch.randint(0, 130, (1, seq_len)).to(self.device)

                with torch.no_grad():
                    start = time.time()
                    logits = model(token_ids, chord_ids, instrument_ids)
                    elapsed = time.time() - start

                shape_correct = logits.shape == (1, seq_len, vocab_size)
                no_nan = not torch.isnan(logits).any()

                results.append(DeepCheckResult(
                    name=f"åºåˆ—é•¿åº¦ {seq_len}",
                    passed=shape_correct and no_nan,
                    message=f"é€šè¿‡ ({elapsed*1000:.1f}ms)" if (shape_correct and no_nan) else "å¤±è´¥",
                ))
                self.add_result(results[-1])

                del model, token_ids, chord_ids, instrument_ids, logits
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            except Exception as e:
                results.append(DeepCheckResult(
                    name=f"åºåˆ—é•¿åº¦ {seq_len}",
                    passed=False,
                    message=f"å¼‚å¸¸: {str(e)[:50]}",
                    severity="error",
                ))
                self.add_result(results[-1])

        # 4.2 RoPE ä½ç½®ç¼–ç æ£€æŸ¥
        try:
            rope = RotaryPositionEmbedding(64, 4096)
            test_input = torch.randn(2, 4096, 256)

            cos, sin = rope(test_input, 4096)

            # æ£€æŸ¥å½¢çŠ¶
            shape_ok = cos.shape == (1, 1, 4096, 64) and sin.shape == (1, 1, 4096, 64)
            # æ£€æŸ¥æ•°å€¼
            no_nan = not (torch.isnan(cos).any() or torch.isnan(sin).any())

            results.append(DeepCheckResult(
                name="RoPE ä½ç½®ç¼–ç  (4096 é•¿åº¦)",
                passed=shape_ok and no_nan,
                message=f"å½¢çŠ¶æ­£ç¡®, æ—  NaN" if (shape_ok and no_nan) else "æ£€æŸ¥å¤±è´¥",
            ))
            self.add_result(results[-1])

        except Exception as e:
            results.append(DeepCheckResult(
                name="RoPE ä½ç½®ç¼–ç ",
                passed=False,
                message=f"å¼‚å¸¸: {str(e)}",
                severity="error",
            ))
            self.add_result(results[-1])

        return results

    # ==================== 5. æ¨¡å‹åˆå§‹åŒ–æ£€æŸ¥ ====================

    def check_model_initialization(self) -> List[DeepCheckResult]:
        """æ£€æŸ¥æ¨¡å‹åˆå§‹åŒ–"""
        self.log("\n" + "=" * 60)
        self.log("5. æ¨¡å‹åˆå§‹åŒ–æ£€æŸ¥")
        self.log("=" * 60)

        results = []
        vocab_size = 643

        model = create_model(vocab_size=vocab_size, model_size='base')

        # 5.1 æƒé‡åˆå§‹åŒ–åˆ†å¸ƒ
        linear_weights = []
        layer_norm_weights = []
        embedding_weights = []

        for name, param in model.named_parameters():
            if 'weight' in name:
                if 'norm' in name.lower():
                    layer_norm_weights.append((name, param))
                elif 'embedding' in name.lower():
                    embedding_weights.append((name, param))
                elif 'proj' in name or 'fc' in name or 'linear' in name or 'lm_head' in name:
                    linear_weights.append((name, param))

        # æ£€æŸ¥ Linear æƒé‡ (åº”è¯¥æ¥è¿‘ N(0, 0.02))
        if linear_weights:
            means = [p.mean().item() for _, p in linear_weights]
            stds = [p.std().item() for _, p in linear_weights]

            avg_mean = sum(abs(m) for m in means) / len(means)
            avg_std = sum(stds) / len(stds)

            mean_ok = avg_mean < 0.01
            std_ok = 0.015 < avg_std < 0.03

            results.append(DeepCheckResult(
                name="Linear æƒé‡åˆå§‹åŒ–",
                passed=mean_ok and std_ok,
                message=f"meanâ‰ˆ{avg_mean:.4f}, stdâ‰ˆ{avg_std:.4f}" +
                        (" (æ­£å¸¸)" if mean_ok and std_ok else " (å¼‚å¸¸)"),
            ))
            self.add_result(results[-1])

        # æ£€æŸ¥ LayerNorm æƒé‡ (åº”è¯¥æ¥è¿‘ 1)
        if layer_norm_weights:
            ln_means = [p.mean().item() for _, p in layer_norm_weights]
            avg_ln_mean = sum(ln_means) / len(ln_means)

            ln_ok = 0.99 < avg_ln_mean < 1.01

            results.append(DeepCheckResult(
                name="LayerNorm æƒé‡åˆå§‹åŒ–",
                passed=ln_ok,
                message=f"meanâ‰ˆ{avg_ln_mean:.4f}" + (" (æ­£å¸¸)" if ln_ok else " (åº”æ¥è¿‘1)"),
            ))
            self.add_result(results[-1])

        # 5.2 åç½®åˆå§‹åŒ– (åº”è¯¥ä¸º 0)
        biases = [(n, p) for n, p in model.named_parameters() if 'bias' in n]
        if biases:
            bias_means = [p.mean().item() for _, p in biases]
            avg_bias = sum(abs(m) for m in bias_means) / len(bias_means)

            bias_ok = avg_bias < 0.001

            results.append(DeepCheckResult(
                name="Bias åˆå§‹åŒ–",
                passed=bias_ok,
                message=f"meanâ‰ˆ{avg_bias:.6f}" + (" (æ¥è¿‘0)" if bias_ok else " (åº”ä¸º0)"),
            ))
            self.add_result(results[-1])

        # 5.3 åµŒå…¥å±‚åˆå§‹åŒ–
        if hasattr(model, 'chord_embedding'):
            embed = model.chord_embedding.token_embedding.weight
            embed_std = embed.std().item()

            embed_ok = 0.015 < embed_std < 0.03

            results.append(DeepCheckResult(
                name="Token åµŒå…¥åˆå§‹åŒ–",
                passed=embed_ok,
                message=f"stdâ‰ˆ{embed_std:.4f}" + (" (æ­£å¸¸)" if embed_ok else " (å¼‚å¸¸)"),
            ))
            self.add_result(results[-1])

        # 5.4 å‚æ•°é‡ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        results.append(DeepCheckResult(
            name="å‚æ•°é‡ç»Ÿè®¡",
            passed=total_params == trainable_params,
            message=f"æ€»è®¡ {total_params/1e6:.2f}M, å¯è®­ç»ƒ {trainable_params/1e6:.2f}M",
        ))
        self.add_result(results[-1])

        return results

    # ==================== 6. è®­ç»ƒè„šæœ¬å…¼å®¹æ€§æ£€æŸ¥ ====================

    def check_training_script_compatibility(self) -> List[DeepCheckResult]:
        """æ£€æŸ¥è®­ç»ƒè„šæœ¬å…¼å®¹æ€§"""
        self.log("\n" + "=" * 60)
        self.log("6. è®­ç»ƒè„šæœ¬å…¼å®¹æ€§æ£€æŸ¥")
        self.log("=" * 60)

        results = []

        # 6.1 æ£€æŸ¥è®­ç»ƒè„šæœ¬å­˜åœ¨
        train_script = Path(__file__).parent / "train.py"
        results.append(DeepCheckResult(
            name="è®­ç»ƒè„šæœ¬å­˜åœ¨",
            passed=train_script.exists(),
            message=str(train_script) if train_script.exists() else "æ‰¾ä¸åˆ° train.py",
        ))
        self.add_result(results[-1])

        if not train_script.exists():
            return results

        # 6.2 æ£€æŸ¥å¯¼å…¥
        try:
            # å°è¯•å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„æ¨¡å—
            from model import HIDMuseFormer, create_model
            from data import HIDMusicDataset, collate_fn, HIDTokenizerV2

            results.append(DeepCheckResult(
                name="æ¨¡å—å¯¼å…¥",
                passed=True,
                message="model å’Œ data æ¨¡å—å¯¼å…¥æˆåŠŸ",
            ))
            self.add_result(results[-1])

        except ImportError as e:
            results.append(DeepCheckResult(
                name="æ¨¡å—å¯¼å…¥",
                passed=False,
                message=f"å¯¼å…¥å¤±è´¥: {str(e)}",
                severity="error",
            ))
            self.add_result(results[-1])

        # 6.3 æ£€æŸ¥ Dataset å’Œ Model æ¥å£å…¼å®¹æ€§
        try:
            tokenizer = HIDTokenizerV2()
            model = create_model(
                vocab_size=tokenizer.vocab_size,
                model_size='small',
                chord_start_id=tokenizer.chord_start_id,
                chord_end_id=tokenizer.chord_end_id,
            )

            results.append(DeepCheckResult(
                name="Model åˆ›å»º (ä½¿ç”¨ tokenizer å‚æ•°)",
                passed=True,
                message=f"vocab_size={tokenizer.vocab_size}",
            ))
            self.add_result(results[-1])

        except Exception as e:
            results.append(DeepCheckResult(
                name="Model åˆ›å»º",
                passed=False,
                message=f"å¤±è´¥: {str(e)}",
                severity="error",
            ))
            self.add_result(results[-1])

        # 6.4 æ£€æŸ¥ collate_fn è¾“å‡ºæ ¼å¼
        try:
            # åˆ›å»ºæ¨¡æ‹Ÿ batch
            tokenizer = HIDTokenizerV2()
            test_content = "#P0\nB0 T0 P60 L4 V16"
            ids, infos = tokenizer.encode_with_info(test_content)

            mock_item = {
                'token_ids': torch.tensor(ids),
                'chord_ids': torch.tensor([i.chord_idx for i in infos]),
                'position_ids': torch.tensor([i.position for i in infos]),
                'instrument_ids': torch.tensor([i.instrument_id if i.instrument_id >= 0 else 129 for i in infos]),
                'is_chord_token': torch.tensor([i.is_chord for i in infos]),
                'length': len(ids),
            }

            batch = collate_fn([mock_item, mock_item])

            required_keys = ['token_ids', 'labels', 'chord_ids', 'instrument_ids', 'key_padding_mask']
            missing = [k for k in required_keys if k not in batch]

            results.append(DeepCheckResult(
                name="collate_fn è¾“å‡ºæ ¼å¼",
                passed=len(missing) == 0,
                message="åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ" if not missing else f"ç¼ºå°‘: {missing}",
            ))
            self.add_result(results[-1])

        except Exception as e:
            results.append(DeepCheckResult(
                name="collate_fn æµ‹è¯•",
                passed=False,
                message=f"å¤±è´¥: {str(e)}",
                severity="error",
            ))
            self.add_result(results[-1])

        # 6.5 æ£€æŸ¥æ··åˆç²¾åº¦å…¼å®¹æ€§
        if torch.cuda.is_available():
            try:
                from torch.cuda.amp import autocast, GradScaler

                model = create_model(vocab_size=643, model_size='small').to(self.device)
                scaler = GradScaler()

                token_ids = torch.randint(0, 643, (2, 64)).to(self.device)

                with autocast():
                    logits = model(token_ids)
                    loss = logits.sum()

                scaler.scale(loss).backward()

                results.append(DeepCheckResult(
                    name="æ··åˆç²¾åº¦ (AMP)",
                    passed=True,
                    message="autocast + GradScaler æ­£å¸¸",
                ))
                self.add_result(results[-1])

            except Exception as e:
                results.append(DeepCheckResult(
                    name="æ··åˆç²¾åº¦ (AMP)",
                    passed=False,
                    message=f"å¤±è´¥: {str(e)}",
                    severity="warning",
                ))
                self.add_result(results[-1])
        else:
            results.append(DeepCheckResult(
                name="æ··åˆç²¾åº¦ (AMP)",
                passed=True,
                message="æ—  GPU, è·³è¿‡æ£€æŸ¥",
                severity="info",
            ))
            self.add_result(results[-1])

        return results

    # ==================== 7. Summary Attention æ·±å…¥æ£€æŸ¥ ====================

    def check_summary_attention_deep(self) -> List[DeepCheckResult]:
        """Summary Attention æ·±å…¥æ£€æŸ¥"""
        self.log("\n" + "=" * 60)
        self.log("7. SUMMARY ATTENTION æ·±å…¥æ£€æŸ¥")
        self.log("=" * 60)

        results = []

        embed_dim = 256
        num_heads = 4
        num_bars = 8
        tokens_per_bar = 32
        batch_size = 2
        reg_len = num_bars * tokens_per_bar
        sum_len = num_bars

        # 7.1 ä¿¡æ¯æµéªŒè¯: SR â†’ K2V2 â†’ RS
        try:
            sum_attn = SummaryAttention(
                embed_dim=embed_dim,
                num_heads=num_heads,
                dropout=0.0,
                use_rope=False,  # ç¦ç”¨ RoPE ä»¥ä¾¿æ›´æ¸…æ¥šåœ°çœ‹åˆ°ä¿¡æ¯æµ
            )

            mask_gen = SummaryAttentionMask()
            bar_ids = torch.arange(reg_len).unsqueeze(0).expand(batch_size, -1) // tokens_per_bar
            instrument_ids = torch.zeros(batch_size, reg_len, dtype=torch.long)

            masks = mask_gen.create_masks(
                bar_ids=bar_ids,
                instrument_ids=instrument_ids,
                num_bars=num_bars,
                device='cpu',
            )

            # SR ä¿¡æ¯éš”ç¦»æµ‹è¯•: æ¯”è¾ƒ bar0 æœ‰ä¿¡æ¯å’Œæ— ä¿¡æ¯æ—¶å„ S çš„è¾“å‡ºå·®å¼‚
            # åŸºå‡†: bar0 = 0
            reg_x_baseline = torch.zeros(batch_size, reg_len, embed_dim)
            sum_x_test = torch.zeros(batch_size, sum_len, embed_dim)

            with torch.no_grad():
                sum_out_baseline, _ = sum_attn(
                    sum_x_test, reg_x_baseline,
                    masks['ss'], masks['sr'], masks['rs'], masks['rr']
                )

            # æµ‹è¯•: bar0 = 1
            reg_x_with_bar0 = torch.zeros(batch_size, reg_len, embed_dim)
            reg_x_with_bar0[:, :tokens_per_bar, :] = 1.0  # bar 0 çš„ token éƒ½æ˜¯ 1

            with torch.no_grad():
                sum_out_with_bar0, _ = sum_attn(
                    sum_x_test, reg_x_with_bar0,
                    masks['ss'], masks['sr'], masks['rs'], masks['rr']
                )

            # S0 åº”è¯¥å—åˆ° bar0 å½±å“
            s0_diff = (sum_out_baseline[0, 0] - sum_out_with_bar0[0, 0]).abs().sum()
            s0_has_bar0_influence = s0_diff > 0.1

            # S1-S7 ä¸åº”è¯¥å—åˆ° bar0 å½±å“ (åªå—å„è‡ª bar å½±å“)
            s1_7_no_bar0_influence = True
            for s_idx in range(1, num_bars):
                s_diff = (sum_out_baseline[0, s_idx] - sum_out_with_bar0[0, s_idx]).abs().sum()
                if s_diff > 1e-5:
                    s1_7_no_bar0_influence = False
                    break

            results.append(DeepCheckResult(
                name="SR ä¿¡æ¯ä¼ æ’­ (S0 å— bar0 å½±å“)",
                passed=s0_has_bar0_influence,
                message=f"S0 è¾“å‡ºå·®å¼‚={s0_diff:.4f}",
            ))
            self.add_result(results[-1])

            results.append(DeepCheckResult(
                name="SR ä¿¡æ¯éš”ç¦» (S1-S7 ä¸å— bar0 å½±å“)",
                passed=s1_7_no_bar0_influence,
                message="æ­£ç¡®éš”ç¦»" if s1_7_no_bar0_influence else "ä¿¡æ¯æ³„éœ²",
            ))
            self.add_result(results[-1])

            # 7.2 RS å› æœæ€§æµ‹è¯•: æ¯”è¾ƒ S0=0 å’Œ S0=1 æ—¶å„ bar çš„è¾“å‡ºå·®å¼‚
            # æ­£ç¡®çš„æµ‹è¯•æ–¹æ³•ï¼šå¦‚æœ bar0 ä¸èƒ½çœ‹åˆ° S0ï¼Œåˆ™ S0 çš„å˜åŒ–ä¸åº”å½±å“ bar0 çš„è¾“å‡º

            # åŸºå‡†: S0 = 0
            sum_x_baseline = torch.zeros(batch_size, sum_len, embed_dim)
            reg_x_test = torch.zeros(batch_size, reg_len, embed_dim)

            with torch.no_grad():
                _, reg_out_baseline = sum_attn(
                    sum_x_baseline, reg_x_test,
                    masks['ss'], masks['sr'], masks['rs'], masks['rr']
                )

            # æµ‹è¯•: S0 = 1
            sum_x_with_s0 = torch.zeros(batch_size, sum_len, embed_dim)
            sum_x_with_s0[:, 0, :] = 1.0  # åªæœ‰ S0 æœ‰ä¿¡æ¯

            with torch.no_grad():
                _, reg_out_with_s0 = sum_attn(
                    sum_x_with_s0, reg_x_test,
                    masks['ss'], masks['sr'], masks['rs'], masks['rr']
                )

            # æ¯”è¾ƒ bar0 çš„è¾“å‡ºå·®å¼‚ï¼ˆå› æœæ€§æµ‹è¯•ï¼‰
            bar0_diff = (reg_out_baseline[0, :tokens_per_bar] -
                        reg_out_with_s0[0, :tokens_per_bar]).abs().sum()
            bar0_no_s0_influence = bar0_diff < 1e-5

            # æ¯”è¾ƒ bar1+ çš„è¾“å‡ºå·®å¼‚ï¼ˆä¿¡æ¯ä¼ æ’­æµ‹è¯•ï¼‰
            bar1_diff = (reg_out_baseline[0, tokens_per_bar:] -
                        reg_out_with_s0[0, tokens_per_bar:]).abs().sum()
            bar1_has_s0_influence = bar1_diff > 0.1

            results.append(DeepCheckResult(
                name="RS ä¿¡æ¯ä¼ æ’­ (bar1+ å— S0 å½±å“)",
                passed=bar1_has_s0_influence,
                message=f"bar1+ è¾“å‡ºå·®å¼‚={bar1_diff:.4f}",
            ))
            self.add_result(results[-1])

            results.append(DeepCheckResult(
                name="RS å› æœæ€§ (bar0 ä¸å— S0 å½±å“)",
                passed=bar0_no_s0_influence,
                message=f"bar0 è¾“å‡ºå·®å¼‚={bar0_diff:.6f}" if bar0_no_s0_influence else f"å› æœæ€§ç ´å: diff={bar0_diff:.4f}",
                severity="critical" if not bar0_no_s0_influence else "info",
            ))
            self.add_result(results[-1])

        except Exception as e:
            results.append(DeepCheckResult(
                name="Summary Attention ä¿¡æ¯æµæ£€æŸ¥",
                passed=False,
                message=f"å¼‚å¸¸: {str(e)}",
                severity="error",
            ))
            self.add_result(results[-1])

        # 7.3 K2V2 äºŒæ¬¡æŠ•å½±éªŒè¯
        try:
            # æ£€æŸ¥ K2V2 æ˜¯å¦çœŸçš„è¢«ä½¿ç”¨
            sum_attn = SummaryAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.0)

            # ä¿å­˜åˆå§‹ K2V2 æƒé‡
            k2_weight_before = sum_attn.sum_k2_proj.weight.clone()
            v2_weight_before = sum_attn.sum_v2_proj.weight.clone()

            # è®­ç»ƒä¸€æ­¥
            sum_x = torch.randn(batch_size, sum_len, embed_dim, requires_grad=True)
            reg_x = torch.randn(batch_size, reg_len, embed_dim, requires_grad=True)

            optimizer = torch.optim.Adam(sum_attn.parameters(), lr=0.01)
            optimizer.zero_grad()

            sum_out, reg_out = sum_attn(
                sum_x, reg_x,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )
            loss = sum_out.sum() + reg_out.sum()
            loss.backward()
            optimizer.step()

            # æ£€æŸ¥ K2V2 æƒé‡æ˜¯å¦å˜åŒ–
            k2_changed = not torch.allclose(k2_weight_before, sum_attn.sum_k2_proj.weight)
            v2_changed = not torch.allclose(v2_weight_before, sum_attn.sum_v2_proj.weight)

            results.append(DeepCheckResult(
                name="K2V2 æƒé‡æ›´æ–°",
                passed=k2_changed and v2_changed,
                message=f"K2 æ›´æ–°: {k2_changed}, V2 æ›´æ–°: {v2_changed}",
            ))
            self.add_result(results[-1])

        except Exception as e:
            results.append(DeepCheckResult(
                name="K2V2 æ£€æŸ¥",
                passed=False,
                message=f"å¼‚å¸¸: {str(e)}",
                severity="error",
            ))
            self.add_result(results[-1])

        return results

    # ==================== è¿è¡Œæ‰€æœ‰æ£€æŸ¥ ====================

    def run_all_checks(self, data_path: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ·±å…¥æ£€æŸ¥"""
        self.log("\n" + "=" * 70)
        self.log("HID-MUSEFORMER æ·±å…¥è®­ç»ƒå‡†å¤‡æ£€æŸ¥")
        self.log("=" * 70)

        start_time = time.time()

        all_results = []

        all_results.extend(self.check_tokenizer_edge_cases())
        all_results.extend(self.check_data_distribution(data_path))
        all_results.extend(self.check_gradient_stability())
        all_results.extend(self.check_long_sequence_handling())
        all_results.extend(self.check_model_initialization())
        all_results.extend(self.check_training_script_compatibility())
        all_results.extend(self.check_summary_attention_deep())

        elapsed = time.time() - start_time

        # ç»Ÿè®¡
        passed = sum(1 for r in all_results if r.passed)
        failed = sum(1 for r in all_results if not r.passed)
        critical = sum(1 for r in all_results if not r.passed and r.severity == "critical")
        errors = sum(1 for r in all_results if not r.passed and r.severity == "error")
        warnings = sum(1 for r in all_results if not r.passed and r.severity == "warning")
        total = len(all_results)

        # æœ€ç»ˆæŠ¥å‘Š
        self.log("\n" + "=" * 70)
        self.log("æ·±å…¥æ£€æŸ¥å®Œæˆ!")
        self.log("=" * 70)
        self.log(f"é€šè¿‡: {passed}/{total}")
        self.log(f"å¤±è´¥: {failed}/{total}")
        if critical > 0:
            self.log(f"  ğŸ’€ è‡´å‘½é—®é¢˜: {critical}")
        if errors > 0:
            self.log(f"  âœ— é”™è¯¯: {errors}")
        if warnings > 0:
            self.log(f"  âš  è­¦å‘Š: {warnings}")
        self.log(f"è€—æ—¶: {elapsed:.2f}s")

        if critical > 0:
            self.log("\nğŸ’€ å‘ç°è‡´å‘½é—®é¢˜! å¿…é¡»ä¿®å¤åæ‰èƒ½è®­ç»ƒã€‚")
        elif errors > 0:
            self.log("\nâœ— å‘ç°é”™è¯¯! å»ºè®®ä¿®å¤åå†è®­ç»ƒã€‚")
        elif warnings > 0:
            self.log("\nâš  å‘ç°è­¦å‘Šã€‚å¯ä»¥è®­ç»ƒï¼Œä½†å»ºè®®å…³æ³¨ã€‚")
        else:
            self.log("\nâœ“ æ‰€æœ‰æ·±å…¥æ£€æŸ¥é€šè¿‡! å¯ä»¥å®‰å…¨å¼€å§‹è®­ç»ƒã€‚")

        return {
            'passed': passed,
            'failed': failed,
            'critical': critical,
            'errors': errors,
            'warnings': warnings,
            'total': total,
            'elapsed': elapsed,
            'ready_to_train': critical == 0 and errors == 0,
            'results': [
                {
                    'name': r.name,
                    'passed': r.passed,
                    'message': r.message,
                    'severity': r.severity,
                }
                for r in all_results
            ],
        }


def main():
    import argparse

    parser = argparse.ArgumentParser(description='æ·±å…¥è®­ç»ƒå‡†å¤‡æ£€æŸ¥')
    parser.add_argument('--data-path', type=str, help='æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, help='è¾“å‡º JSON æŠ¥å‘Šè·¯å¾„')
    parser.add_argument('--quiet', action='store_true', help='å®‰é™æ¨¡å¼')

    args = parser.parse_args()

    checker = DeepTrainingChecker(verbose=not args.quiet)
    report = checker.run_all_checks(data_path=args.data_path)

    if args.output:
        with open(args.output, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")

    sys.exit(0 if report['ready_to_train'] else 1)


if __name__ == '__main__':
    main()
