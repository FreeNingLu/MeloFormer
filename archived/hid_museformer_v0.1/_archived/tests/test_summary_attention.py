#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Summary Token æ³¨æ„åŠ›æœºåˆ¶ç»¼åˆæµ‹è¯•

æµ‹è¯•å†…å®¹ï¼š
1. æ©ç æ­£ç¡®æ€§æµ‹è¯•
2. æ¢¯åº¦æµåŠ¨æµ‹è¯•
3. ä¿¡æ¯ä¼ é€’æµ‹è¯•
4. è®¡ç®—å¤æ‚åº¦æµ‹è¯•
5. å¤šå±‚å †å æµ‹è¯•
6. è¾¹ç•Œæƒ…å†µæµ‹è¯•
7. å› æœæ€§éªŒè¯
8. æ•°å€¼ç¨³å®šæ€§æµ‹è¯•
"""

import torch
import torch.nn as nn
import time
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from attention import (
    SummaryAttention,
    SummaryAttentionBlock,
    SummaryAttentionMask,
    SummaryTokenEmbedding,
    FCAttentionMask,
)


def print_header(title: str):
    """æ‰“å°æµ‹è¯•æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}")


def print_result(name: str, passed: bool, details: str = ""):
    """æ‰“å°æµ‹è¯•ç»“æœ"""
    status = "âœ“ PASS" if passed else "âœ— FAIL"
    print(f"  {status}: {name}")
    if details:
        print(f"         {details}")


class TestSummaryAttention:
    """Summary Token æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•ç±»"""

    def __init__(self, embed_dim=256, num_heads=8, num_bars=8, tokens_per_bar=12):
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_bars = num_bars
        self.tokens_per_bar = tokens_per_bar
        self.reg_len = num_bars * tokens_per_bar
        self.sum_len = num_bars

        # åˆ›å»ºæ¨¡å—
        self.sum_embedding = SummaryTokenEmbedding(embed_dim, max_bars=256)
        self.mask_generator = SummaryAttentionMask()
        self.attention = SummaryAttention(embed_dim, num_heads, dropout=0.0)
        self.block = SummaryAttentionBlock(embed_dim, num_heads, embed_dim * 4, dropout=0.0)

    def _create_test_data(self, batch_size=2):
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        # Regular token åµŒå…¥
        reg_x = torch.randn(batch_size, self.reg_len, self.embed_dim)

        # Bar IDs: æ¯ tokens_per_bar ä¸ª token å±äºä¸€ä¸ª bar
        bar_ids = torch.arange(self.num_bars).repeat_interleave(self.tokens_per_bar)
        bar_ids = bar_ids.unsqueeze(0).expand(batch_size, -1)

        # Instrument IDs
        instrument_ids = torch.zeros(batch_size, self.reg_len, dtype=torch.long)

        # Summary token åµŒå…¥
        sum_x = self.sum_embedding(self.num_bars, batch_size, reg_x.device)

        # åˆ›å»ºæ©ç 
        masks = self.mask_generator.create_masks(
            bar_ids, instrument_ids, self.num_bars, reg_x.device
        )

        return reg_x, sum_x, bar_ids, instrument_ids, masks

    def test_mask_correctness(self) -> bool:
        """æµ‹è¯• 1: æ©ç æ­£ç¡®æ€§"""
        print_header("æµ‹è¯• 1: æ©ç æ­£ç¡®æ€§")

        batch_size = 2
        reg_x, sum_x, bar_ids, instrument_ids, masks = self._create_test_data(batch_size)

        all_passed = True

        # 1.1 SS æ©ç åº”è¯¥æ˜¯å› æœçš„
        ss_expected = torch.tril(torch.ones(self.sum_len, self.sum_len)).bool()
        ss_expected = ss_expected.unsqueeze(0).expand(batch_size, -1, -1)
        ss_correct = torch.all(masks['ss'] == ss_expected).item()
        print_result("SS æ©ç å› æœæ€§", ss_correct)
        all_passed = all_passed and ss_correct

        # 1.2 SR æ©ç : S_i åªèƒ½çœ‹ bar i çš„ R
        sr_correct = True
        for b in range(batch_size):
            for bar_idx in range(self.num_bars):
                expected = (bar_ids[b] == bar_idx)
                actual = masks['sr'][b, bar_idx]
                if not torch.all(expected == actual):
                    sr_correct = False
                    break
        print_result("SR æ©ç æ­£ç¡®æ€§", sr_correct,
                    f"S_i åªèƒ½çœ‹ bar i çš„ R")
        all_passed = all_passed and sr_correct

        # 1.3 RS æ©ç : R åªèƒ½çœ‹å·²å®Œæˆ bar çš„ S
        rs_correct = True
        for b in range(batch_size):
            for j in range(self.reg_len):
                r_bar = bar_ids[b, j].item()
                expected = torch.zeros(self.num_bars, dtype=torch.bool)
                expected[:r_bar] = True
                actual = masks['rs'][b, j]
                if not torch.all(expected == actual):
                    rs_correct = False
                    break
        print_result("RS æ©ç æ­£ç¡®æ€§", rs_correct,
                    f"R åªèƒ½çœ‹å·²å®Œæˆ bar çš„ S")
        all_passed = all_passed and rs_correct

        # 1.4 RR æ©ç åº”è¯¥æ˜¯å› æœçš„
        rr_causal = torch.all(
            masks['rr'] == (masks['rr'] & torch.tril(torch.ones_like(masks['rr'][0])))
        ).item()
        print_result("RR æ©ç å› æœæ€§", rr_causal)
        all_passed = all_passed and rr_causal

        return all_passed

    def test_gradient_flow(self) -> bool:
        """æµ‹è¯• 2: æ¢¯åº¦æµåŠ¨"""
        print_header("æµ‹è¯• 2: æ¢¯åº¦æµåŠ¨")

        batch_size = 2
        reg_x, sum_x, bar_ids, instrument_ids, masks = self._create_test_data(batch_size)

        reg_x.requires_grad_(True)
        sum_x = sum_x.detach().requires_grad_(True)

        all_passed = True

        # 2.1 å‰å‘ä¼ æ’­
        sum_out, reg_out = self.attention(
            sum_x, reg_x,
            masks['ss'], masks['sr'], masks['rs'], masks['rr']
        )

        # 2.2 åå‘ä¼ æ’­
        loss = sum_out.sum() + reg_out.sum()
        loss.backward()

        # 2.3 æ£€æŸ¥æ¢¯åº¦
        sum_grad_exists = sum_x.grad is not None and not torch.all(sum_x.grad == 0)
        reg_grad_exists = reg_x.grad is not None and not torch.all(reg_x.grad == 0)

        print_result("Summary æ¢¯åº¦å­˜åœ¨", sum_grad_exists)
        print_result("Regular æ¢¯åº¦å­˜åœ¨", reg_grad_exists)
        all_passed = all_passed and sum_grad_exists and reg_grad_exists

        # 2.4 æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰ NaN
        sum_grad_valid = not torch.isnan(sum_x.grad).any().item()
        reg_grad_valid = not torch.isnan(reg_x.grad).any().item()

        print_result("Summary æ¢¯åº¦æ—  NaN", sum_grad_valid)
        print_result("Regular æ¢¯åº¦æ—  NaN", reg_grad_valid)
        all_passed = all_passed and sum_grad_valid and reg_grad_valid

        return all_passed

    def test_information_flow(self) -> bool:
        """æµ‹è¯• 3: ä¿¡æ¯ä¼ é€’"""
        print_header("æµ‹è¯• 3: ä¿¡æ¯ä¼ é€’")

        all_passed = True

        # 3.1 æµ‹è¯• SR: ä¿®æ”¹ bar 0 çš„ Rï¼Œåº”è¯¥åªå½±å“ S_0
        batch_size = 1
        reg_x, sum_x, bar_ids, instrument_ids, masks = self._create_test_data(batch_size)

        # åŸºçº¿è¾“å‡º
        with torch.no_grad():
            sum_base, reg_base = self.attention(
                sum_x, reg_x,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        # ä¿®æ”¹ bar 0 çš„ R
        reg_x_mod = reg_x.clone()
        reg_x_mod[:, :self.tokens_per_bar, :] += 10.0

        with torch.no_grad():
            sum_mod, reg_mod = self.attention(
                sum_x, reg_x_mod,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        # S_0 åº”è¯¥æ”¹å˜æœ€å¤šï¼ˆç›´æ¥å— bar 0 å½±å“ï¼‰
        sum_diff = (sum_mod - sum_base).abs().mean(dim=-1)  # (batch, sum_len)
        s0_changed = sum_diff[0, 0] > 0.01
        print_result("SR ä¿¡æ¯ä¼ é€’: S_0 å— bar 0 å½±å“", s0_changed.item(),
                    f"S_0 å˜åŒ–é‡: {sum_diff[0, 0]:.4f}")
        all_passed = all_passed and s0_changed.item()

        # 3.2 æµ‹è¯• RS: ä¿®æ”¹ S_0ï¼Œbar 1+ çš„ R åº”è¯¥å—å½±å“
        sum_x_mod = sum_x.clone()
        sum_x_mod[:, 0, :] += 10.0

        with torch.no_grad():
            _, reg_mod2 = self.attention(
                sum_x_mod, reg_x,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        reg_diff = (reg_mod2 - reg_base).abs().mean(dim=-1)  # (batch, reg_len)

        # bar 0 çš„ R ä¸åº”è¯¥å— S_0 å½±å“ï¼ˆå› ä¸º RS æ©ç ï¼‰
        bar0_unchanged = reg_diff[0, :self.tokens_per_bar].mean() < 0.01
        # bar 1+ çš„ R åº”è¯¥å—å½±å“
        bar1_plus_changed = reg_diff[0, self.tokens_per_bar:].mean() > 0.01

        print_result("RS ä¿¡æ¯ä¼ é€’: bar 0 ä¸å— S_0 å½±å“", bar0_unchanged.item(),
                    f"bar 0 å˜åŒ–é‡: {reg_diff[0, :self.tokens_per_bar].mean():.4f}")
        print_result("RS ä¿¡æ¯ä¼ é€’: bar 1+ å— S_0 å½±å“", bar1_plus_changed.item(),
                    f"bar 1+ å˜åŒ–é‡: {reg_diff[0, self.tokens_per_bar:].mean():.4f}")

        all_passed = all_passed and bar0_unchanged.item() and bar1_plus_changed.item()

        return all_passed

    def test_computational_complexity(self) -> bool:
        """æµ‹è¯• 4: è®¡ç®—å¤æ‚åº¦"""
        print_header("æµ‹è¯• 4: è®¡ç®—å¤æ‚åº¦")

        all_passed = True

        # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦çš„è®¡ç®—æ—¶é—´
        configs = [
            (4, 10),   # 40 tokens
            (8, 15),   # 120 tokens
            (16, 20),  # 320 tokens
            (32, 25),  # 800 tokens
        ]

        times = []
        for num_bars, tokens_per_bar in configs:
            reg_len = num_bars * tokens_per_bar

            # åˆ›å»ºæ•°æ®
            reg_x = torch.randn(1, reg_len, self.embed_dim)
            bar_ids = torch.arange(num_bars).repeat_interleave(tokens_per_bar).unsqueeze(0)
            instrument_ids = torch.zeros(1, reg_len, dtype=torch.long)
            sum_x = self.sum_embedding(num_bars, 1, reg_x.device)

            # åˆ›å»ºæ©ç 
            mask_gen = SummaryAttentionMask()
            masks = mask_gen.create_masks(bar_ids, instrument_ids, num_bars, reg_x.device)

            # åˆ›å»º attention
            attn = SummaryAttention(self.embed_dim, self.num_heads, dropout=0.0)

            # é¢„çƒ­
            with torch.no_grad():
                attn(sum_x, reg_x, masks['ss'], masks['sr'], masks['rs'], masks['rr'])

            # è®¡æ—¶
            start = time.time()
            num_runs = 10
            with torch.no_grad():
                for _ in range(num_runs):
                    attn(sum_x, reg_x, masks['ss'], masks['sr'], masks['rs'], masks['rr'])
            elapsed = (time.time() - start) / num_runs * 1000

            times.append((reg_len, elapsed))
            print_result(f"åºåˆ—é•¿åº¦ {reg_len}", True, f"å¹³å‡è€—æ—¶: {elapsed:.2f} ms")

        # æ£€æŸ¥å¤æ‚åº¦æ˜¯å¦æ¥è¿‘çº¿æ€§ï¼ˆç›¸å¯¹äº O(nÂ²)ï¼‰
        # å¦‚æœæ˜¯ O(nÂ²)ï¼Œæ—¶é—´åº”è¯¥å¢é•¿ ~(n2/n1)Â²
        # å¦‚æœæ˜¯ O(n)ï¼Œæ—¶é—´åº”è¯¥å¢é•¿ ~(n2/n1)
        ratio_actual = times[-1][1] / times[0][1]
        ratio_len = times[-1][0] / times[0][0]
        ratio_quadratic = ratio_len ** 2

        # FC-Attention åº”è¯¥æ¯”çº¯ O(nÂ²) å¥½
        is_subquadratic = ratio_actual < ratio_quadratic * 0.8
        print_result("å¤æ‚åº¦ä¼˜äº O(nÂ²)", is_subquadratic,
                    f"å®é™…å¢é•¿: {ratio_actual:.1f}x, O(nÂ²)é¢„æœŸ: {ratio_quadratic:.1f}x")

        all_passed = all_passed and is_subquadratic

        return all_passed

    def test_multi_layer_stacking(self) -> bool:
        """æµ‹è¯• 5: å¤šå±‚å †å """
        print_header("æµ‹è¯• 5: å¤šå±‚å †å ")

        batch_size = 2
        num_layers = 4

        # åˆ›å»ºå¤šå±‚ block
        layers = nn.ModuleList([
            SummaryAttentionBlock(self.embed_dim, self.num_heads, self.embed_dim * 4, dropout=0.0)
            for _ in range(num_layers)
        ])

        reg_x, sum_x, bar_ids, instrument_ids, masks = self._create_test_data(batch_size)
        reg_x.requires_grad_(True)
        sum_x = sum_x.detach().requires_grad_(True)

        all_passed = True

        # å‰å‘ä¼ æ’­
        for layer in layers:
            sum_x, reg_x = layer(
                sum_x, reg_x,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        # æ£€æŸ¥è¾“å‡º
        output_valid = not torch.isnan(sum_x).any() and not torch.isnan(reg_x).any()
        print_result(f"{num_layers} å±‚å †å è¾“å‡ºæœ‰æ•ˆ", output_valid)
        all_passed = all_passed and output_valid

        # åå‘ä¼ æ’­
        loss = sum_x.sum() + reg_x.sum()
        loss.backward()

        # æ£€æŸ¥ç¬¬ä¸€å±‚çš„æ¢¯åº¦
        first_layer_grad_valid = all(
            p.grad is not None and not torch.isnan(p.grad).any()
            for p in layers[0].parameters()
        )
        print_result(f"ç¬¬ä¸€å±‚æ¢¯åº¦æœ‰æ•ˆ", first_layer_grad_valid)
        all_passed = all_passed and first_layer_grad_valid

        return all_passed

    def test_edge_cases(self) -> bool:
        """æµ‹è¯• 6: è¾¹ç•Œæƒ…å†µ"""
        print_header("æµ‹è¯• 6: è¾¹ç•Œæƒ…å†µ")

        all_passed = True

        # 6.1 å•ä¸ª bar
        num_bars = 1
        tokens_per_bar = 10
        reg_len = num_bars * tokens_per_bar

        reg_x = torch.randn(1, reg_len, self.embed_dim)
        bar_ids = torch.zeros(1, reg_len, dtype=torch.long)
        instrument_ids = torch.zeros(1, reg_len, dtype=torch.long)
        sum_x = self.sum_embedding(num_bars, 1, reg_x.device)

        mask_gen = SummaryAttentionMask()
        masks = mask_gen.create_masks(bar_ids, instrument_ids, num_bars, reg_x.device)

        attn = SummaryAttention(self.embed_dim, self.num_heads, dropout=0.0)

        try:
            with torch.no_grad():
                sum_out, reg_out = attn(
                    sum_x, reg_x,
                    masks['ss'], masks['sr'], masks['rs'], masks['rr']
                )
            single_bar_ok = not torch.isnan(sum_out).any() and not torch.isnan(reg_out).any()
        except Exception as e:
            single_bar_ok = False
            print(f"         Error: {e}")

        print_result("å• bar åœºæ™¯", single_bar_ok)
        all_passed = all_passed and single_bar_ok

        # 6.2 ç©º bar (æŸäº› bar æ²¡æœ‰ token)
        # è¿™ç§æƒ…å†µåœ¨å®é™…ä¸­å¯èƒ½ä¸ä¼šå‘ç”Ÿï¼Œä½†åº”è¯¥èƒ½å¤„ç†

        # 6.3 å¤§ batch size
        batch_size = 16
        reg_x, sum_x, bar_ids, instrument_ids, masks = self._create_test_data(batch_size)

        try:
            with torch.no_grad():
                sum_out, reg_out = self.attention(
                    sum_x, reg_x,
                    masks['ss'], masks['sr'], masks['rs'], masks['rr']
                )
            large_batch_ok = not torch.isnan(sum_out).any() and not torch.isnan(reg_out).any()
        except Exception as e:
            large_batch_ok = False
            print(f"         Error: {e}")

        print_result(f"å¤§ batch ({batch_size}) åœºæ™¯", large_batch_ok)
        all_passed = all_passed and large_batch_ok

        return all_passed

    def test_causality(self) -> bool:
        """æµ‹è¯• 7: å› æœæ€§éªŒè¯"""
        print_header("æµ‹è¯• 7: å› æœæ€§éªŒè¯")

        all_passed = True

        batch_size = 1
        reg_x, sum_x, bar_ids, instrument_ids, masks = self._create_test_data(batch_size)

        # 7.1 ä¿®æ”¹æœªæ¥çš„ tokenï¼Œä¸åº”è¯¥å½±å“è¿‡å»çš„è¾“å‡º
        with torch.no_grad():
            sum_base, reg_base = self.attention(
                sum_x, reg_x,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        # ä¿®æ”¹æœ€åä¸€ä¸ª bar çš„ token
        reg_x_mod = reg_x.clone()
        last_bar_start = (self.num_bars - 1) * self.tokens_per_bar
        reg_x_mod[:, last_bar_start:, :] += 100.0

        with torch.no_grad():
            sum_mod, reg_mod = self.attention(
                sum_x, reg_x_mod,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        # é™¤äº†æœ€åä¸€ä¸ª barï¼Œå…¶ä»– bar çš„ R è¾“å‡ºåº”è¯¥ä¸å˜
        reg_diff = (reg_mod - reg_base).abs()
        earlier_bars_unchanged = reg_diff[:, :last_bar_start, :].max() < 1e-5

        print_result("å› æœæ€§: æœªæ¥ä¿®æ”¹ä¸å½±å“è¿‡å» R", earlier_bars_unchanged.item(),
                    f"è¿‡å» bar æœ€å¤§å˜åŒ–: {reg_diff[:, :last_bar_start, :].max():.2e}")
        all_passed = all_passed and earlier_bars_unchanged.item()

        # é™¤äº†æœ€åä¸€ä¸ª Sï¼Œå…¶ä»– S åº”è¯¥ä¸å˜
        sum_diff = (sum_mod - sum_base).abs()
        earlier_s_unchanged = sum_diff[:, :-1, :].max() < 1e-5

        print_result("å› æœæ€§: æœªæ¥ä¿®æ”¹ä¸å½±å“è¿‡å» S", earlier_s_unchanged.item(),
                    f"è¿‡å» S æœ€å¤§å˜åŒ–: {sum_diff[:, :-1, :].max():.2e}")
        all_passed = all_passed and earlier_s_unchanged.item()

        return all_passed

    def test_numerical_stability(self) -> bool:
        """æµ‹è¯• 8: æ•°å€¼ç¨³å®šæ€§"""
        print_header("æµ‹è¯• 8: æ•°å€¼ç¨³å®šæ€§")

        all_passed = True

        batch_size = 2

        # 8.1 å¤§è¾“å…¥å€¼
        reg_x, sum_x, bar_ids, instrument_ids, masks = self._create_test_data(batch_size)
        reg_x_large = reg_x * 100

        with torch.no_grad():
            sum_out, reg_out = self.attention(
                sum_x, reg_x_large,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        large_input_stable = not torch.isnan(sum_out).any() and not torch.isnan(reg_out).any()
        large_input_stable = large_input_stable and not torch.isinf(sum_out).any() and not torch.isinf(reg_out).any()

        print_result("å¤§è¾“å…¥å€¼ç¨³å®šæ€§", large_input_stable)
        all_passed = all_passed and large_input_stable

        # 8.2 å°è¾“å…¥å€¼
        reg_x_small = reg_x * 0.001

        with torch.no_grad():
            sum_out, reg_out = self.attention(
                sum_x, reg_x_small,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        small_input_stable = not torch.isnan(sum_out).any() and not torch.isnan(reg_out).any()

        print_result("å°è¾“å…¥å€¼ç¨³å®šæ€§", small_input_stable)
        all_passed = all_passed and small_input_stable

        # 8.3 åŠç²¾åº¦æµ‹è¯• (å¦‚æœæ”¯æŒ)
        if torch.cuda.is_available():
            device = torch.device('cuda')
            reg_x_fp16 = reg_x.to(device).half()
            sum_x_fp16 = sum_x.to(device).half()
            masks_fp16 = {k: v.to(device) for k, v in masks.items()}

            attn_fp16 = SummaryAttention(self.embed_dim, self.num_heads, dropout=0.0).to(device).half()

            with torch.no_grad():
                sum_out, reg_out = attn_fp16(
                    sum_x_fp16, reg_x_fp16,
                    masks_fp16['ss'], masks_fp16['sr'], masks_fp16['rs'], masks_fp16['rr']
                )

            fp16_stable = not torch.isnan(sum_out).any() and not torch.isnan(reg_out).any()
            print_result("FP16 ç¨³å®šæ€§", fp16_stable)
            all_passed = all_passed and fp16_stable
        else:
            print_result("FP16 ç¨³å®šæ€§", True, "(è·³è¿‡ï¼Œæ—  GPU)")

        return all_passed

    def test_k2v2_effect(self) -> bool:
        """æµ‹è¯• 9: K2/V2 äºŒæ¬¡æŠ•å½±æ•ˆæœ"""
        print_header("æµ‹è¯• 9: K2/V2 äºŒæ¬¡æŠ•å½±æ•ˆæœ")

        all_passed = True
        batch_size = 2

        reg_x, sum_x, bar_ids, instrument_ids, masks = self._create_test_data(batch_size)

        # æ£€æŸ¥ K2, V2 æŠ•å½±æ˜¯å¦èµ·ä½œç”¨
        # å¦‚æœ K2, V2 æ˜¯æ’ç­‰å˜æ¢ï¼Œè¾“å‡ºåº”è¯¥ä¸åŒ

        # åˆ›å»ºä¸€ä¸ª attentionï¼Œæ‰‹åŠ¨è®¾ç½® K2, V2 ä¸ºæ’ç­‰
        attn_identity = SummaryAttention(self.embed_dim, self.num_heads, dropout=0.0)
        with torch.no_grad():
            # è®¾ç½® K2, V2 ä¸ºæ’ç­‰å˜æ¢
            nn.init.eye_(attn_identity.sum_k2_proj.weight)
            nn.init.zeros_(attn_identity.sum_k2_proj.bias)
            nn.init.eye_(attn_identity.sum_v2_proj.weight)
            nn.init.zeros_(attn_identity.sum_v2_proj.bias)

            sum_out_identity, reg_out_identity = attn_identity(
                sum_x, reg_x,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        # åˆ›å»ºæ­£å¸¸çš„ attention
        attn_normal = SummaryAttention(self.embed_dim, self.num_heads, dropout=0.0)
        with torch.no_grad():
            sum_out_normal, reg_out_normal = attn_normal(
                sum_x, reg_x,
                masks['ss'], masks['sr'], masks['rs'], masks['rr']
            )

        # è¾“å‡ºåº”è¯¥ä¸åŒ
        outputs_different = not torch.allclose(reg_out_identity, reg_out_normal, atol=1e-3)

        print_result("K2/V2 æŠ•å½±äº§ç”Ÿä¸åŒè¾“å‡º", outputs_different)
        all_passed = all_passed and outputs_different

        return all_passed

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("  Summary Token æ³¨æ„åŠ›æœºåˆ¶ç»¼åˆæµ‹è¯•")
        print("="*60)
        print(f"\né…ç½®: embed_dim={self.embed_dim}, heads={self.num_heads}, "
              f"bars={self.num_bars}, tokens/bar={self.tokens_per_bar}")

        results = {}

        results['mask'] = self.test_mask_correctness()
        results['gradient'] = self.test_gradient_flow()
        results['info_flow'] = self.test_information_flow()
        results['complexity'] = self.test_computational_complexity()
        results['multi_layer'] = self.test_multi_layer_stacking()
        results['edge_cases'] = self.test_edge_cases()
        results['causality'] = self.test_causality()
        results['stability'] = self.test_numerical_stability()
        results['k2v2'] = self.test_k2v2_effect()

        # æ€»ç»“
        print_header("æµ‹è¯•æ€»ç»“")

        total = len(results)
        passed = sum(results.values())

        for name, result in results.items():
            status = "âœ“" if result else "âœ—"
            print(f"  {status} {name}")

        print(f"\n  é€šè¿‡: {passed}/{total}")

        if passed == total:
            print("\n  ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        else:
            print(f"\n  âš ï¸  {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")

        return passed == total


def test_with_token_types():
    """æµ‹è¯•ä¸ Token ç±»å‹çº§ç¨€ç–çš„é›†æˆ"""
    print_header("é™„åŠ æµ‹è¯•: Token ç±»å‹çº§ç¨€ç–é›†æˆ")

    embed_dim = 256
    num_heads = 8
    num_bars = 4
    tokens_per_bar = 16  # æ¯ä¸ª bar: 4 notes Ã— 4 tokens (T, P, L, V)
    reg_len = num_bars * tokens_per_bar

    # åˆ›å»ºæ•°æ®
    batch_size = 2
    reg_x = torch.randn(batch_size, reg_len, embed_dim)

    # Bar IDs
    bar_ids = torch.arange(num_bars).repeat_interleave(tokens_per_bar)
    bar_ids = bar_ids.unsqueeze(0).expand(batch_size, -1)

    # Instrument IDs
    instrument_ids = torch.zeros(batch_size, reg_len, dtype=torch.long)

    # Token types: 0=T, 1=P, 2=D, 3=V, å¾ªç¯
    token_types = torch.tensor([0, 1, 2, 3] * (tokens_per_bar // 4))
    token_types = token_types.repeat(num_bars)
    token_type_ids = token_types.unsqueeze(0).expand(batch_size, -1)

    # Note IDs: æ¯ 4 ä¸ª token ä¸€ä¸ª note
    note_ids = torch.arange(reg_len // 4).repeat_interleave(4)
    note_ids = note_ids.unsqueeze(0).expand(batch_size, -1)

    # åˆ›å»ºæ©ç 
    mask_gen = SummaryAttentionMask()
    masks = mask_gen.create_masks(
        bar_ids, instrument_ids, num_bars, reg_x.device,
        token_type_ids=token_type_ids,
        note_ids=note_ids,
    )

    # Summary token
    sum_embedding = SummaryTokenEmbedding(embed_dim)
    sum_x = sum_embedding(num_bars, batch_size, reg_x.device)

    # Attention
    attn = SummaryAttention(embed_dim, num_heads, dropout=0.0)

    with torch.no_grad():
        sum_out, reg_out = attn(
            sum_x, reg_x,
            masks['ss'], masks['sr'], masks['rs'], masks['rr']
        )

    output_valid = not torch.isnan(sum_out).any() and not torch.isnan(reg_out).any()
    print_result("Token ç±»å‹çº§ç¨€ç–é›†æˆ", output_valid)

    # æ£€æŸ¥ RR æ©ç ä¸­ D token æ˜¯å¦è¢«éš”ç¦»
    # D token (type=2) åªèƒ½çœ‹åŒéŸ³ç¬¦çš„å…¶ä»– token
    d_indices = (token_type_ids[0] == 2).nonzero().squeeze()
    if d_indices.numel() > 0:
        d_idx = d_indices[4].item()  # å–ç¬¬5ä¸ª D token
        d_mask = masks['rr'][0, d_idx]  # è¿™ä¸ª D token èƒ½çœ‹åˆ°ä»€ä¹ˆ

        # D token åº”è¯¥åªèƒ½çœ‹åˆ°åŒéŸ³ç¬¦çš„ token
        note_id = note_ids[0, d_idx].item()
        same_note_mask = (note_ids[0] == note_id)

        # D token å¯¹å…¶ä»–éŸ³ç¬¦çš„ T, P, D, V åº”è¯¥éƒ½çœ‹ä¸åˆ°
        other_notes_mask = ~same_note_mask
        d_sees_other_notes = d_mask[other_notes_mask].any()

        print_result("D token ä¸çœ‹å…¶ä»–éŸ³ç¬¦", not d_sees_other_notes.item())

    return output_valid


if __name__ == '__main__':
    # è¿è¡Œä¸»æµ‹è¯•
    tester = TestSummaryAttention()
    all_passed = tester.run_all_tests()

    # é™„åŠ æµ‹è¯•
    token_type_test = test_with_token_types()

    # æœ€ç»ˆç»“æœ
    print("\n" + "="*60)
    if all_passed and token_type_test:
        print("  âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Summary Token æ³¨æ„åŠ›æœºåˆ¶å®ç°æ­£ç¡®ã€‚")
    else:
        print("  âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
    print("="*60)
