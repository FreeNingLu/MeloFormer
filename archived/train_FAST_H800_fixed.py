# ‚ö° H800 ‰ºòÂåñËÆ≠ÁªÉÁâà - FIXED VERSION
"""
üöÄ H800 ‰ºòÂåñÈÖçÁΩÆÔºà80GBÊòæÂ≠òÔºâÔºö

Á°¨‰ª∂ËßÑÊ†ºÔºö
- GPU: H800 80GB (HopperÊû∂ÊûÑ)
- È©±Âä®: 570.124.04
- CUDA: 12.8
- CPU: 20Ê†∏ Xeon Platinum 8458P
- ÂÜÖÂ≠ò: 100GB

üîß ÂÖ≥ÈîÆ‰øÆÂ§çÔºàÂü∫‰∫éGPTÂª∫ËÆÆÔºâÔºö

1. ‚úÖ ‰øÆÂ§ç Padding Mask BugÔºàÊúÄÈáçË¶ÅÔºâ
   - ÈóÆÈ¢òÔºöpaddingÁöÑ0Ë¢´ÂΩìÊàêÊ≠£Â∏∏tokenÂèÇ‰∏élossËÆ°ÁÆó
   - ‰øÆÂ§çÔºöCrossEntropyLossËÆæÁΩÆignore_index=0
   - È¢ÑÊúüÔºöloss‰∏ãÈôçÊõ¥Âø´ÔºåÁîüÊàêË¥®ÈáèÊèêÂçá

2. ‚úÖ ÂÖ≥Èó≠ Label Smoothing
   - ÈóÆÈ¢òÔºö0.1ÁöÑsmoothingÂú®30k vocab‰∏ãËøáÂ§ßÔºåÂØºËá¥Ê¨†ÊãüÂêà
   - ‰øÆÂ§çÔºölabel_smoothing‰ªé0.1Êîπ‰∏∫0
   - È¢ÑÊúüÔºöÊ®°ÂûãÊõ¥ÂÆπÊòìÂ≠¶Âà∞Á°ÆÂÆöÊÄßÈ¢ÑÊµã

3. ‚úÖ Â¢ûÂä†ËÆ≠ÁªÉÈáè
   - ÈóÆÈ¢òÔºö10 epochsÂ§™Â∞ëÔºåÊ®°ÂûãÊ¨†ÊãüÂêà
   - ‰øÆÂ§çÔºöepochs‰ªé10ÊèêÈ´òÂà∞20Ôºålearning_rate‰ªé5e-6ÊèêÈ´òÂà∞1e-5
   - È¢ÑÊúüÔºöÊ®°ÂûãÂÖÖÂàÜÂ≠¶‰π†MIDIÊó∂Â∫èÁªìÊûÑ

üöÄ H800 ÊÄßËÉΩ‰ºòÂåñÔºàÁõ∏ÊØîRTX 5090ÔºâÔºö

1. ‚úÖ Â¢ûÂ§ßbatch sizeÔºà2‚Üí8ÔºåÂÖÖÂàÜÂà©Áî®80GBÊòæÂ≠òÔºâ
2. ‚úÖ ÂáèÂ∞ëgradient accumulationÔºà32‚Üí8Ôºå‰øùÊåÅÊúâÊïàbatch=64Ôºâ
3. ‚úÖ Â¢ûÂ§ßÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶Ôºà15000‚Üí15000ÔºåÂ§ÑÁêÜÊõ¥ÈïøMIDIÔºâ
4. ‚úÖ Â¢ûÂä†Êï∞ÊçÆÂä†ËΩΩworkersÔºà8‚Üí16Ôºå20Ê†∏CPUÔºâ
5. ‚úÖ Flash Attention 2ÔºàHopperÊû∂ÊûÑÂéüÁîü‰ºòÂåñÔºâ
6. ‚úÖ ‰ΩøÁî®BF16Ê∑∑ÂêàÁ≤æÂ∫¶ËÆ≠ÁªÉ
7. ‚úÖ ÊèêÈ´òÂ≠¶‰π†ÁéáÔºà5e-6‚Üí1e-5Ôºâ

È¢ÑËÆ°ÊïàÊûúÔºö
- ËÆ≠ÁªÉÊó∂Èó¥ÔºöÁ∫¶3-4Â§©Ôºà20 epochsÔºåÊØîRTX 5090Âø´50%+Ôºâ
- Loss‰∏ãÈôçÔºöÊõ¥Âø´„ÄÅÊõ¥Á®≥ÂÆö
- ÁîüÊàêË¥®ÈáèÔºöÊòæËëóÊèêÂçá
- ÊîØÊåÅÊõ¥ÈïøMIDIÂ∫èÂàóÔºà15000 tokens vs 15000Ôºâ


python train_FAST_H800_fixed.py \
    --caption_path /root/autodl-tmp/tmp/captions/captionMidiCapsPlus.json \
    --midi_folder /root/autodl-tmp/tmp/MidiCaps \
    --output_dir output_H800 \
    --epochs 20

    python train_FAST_H800_fixed.py --caption_path /root/autodl-tmp/Text2midi/captions/captionMidiCapsPlus.json --midi_folder /root/autodl-tmp/Text2midi/MidiCaps --output_dir /root/autodl-tmp/Text2midi/output_MidiCaps_H800_FIXED > training.log 2>&1 &

"""

import os
import torch.nn as nn
import torch.optim as optim
import yaml
import math
import time
from transformers import get_scheduler
import wandb
import pickle
import numpy as np
import json
import jsonlines
from tqdm import tqdm
import torch
from accelerate import Accelerator
from accelerate.logging import get_logger
import logging
import argparse

# ÂØºÂÖ•Ê®°ÂûãÂíåÊï∞ÊçÆÂä†ËΩΩÂô®
import sys
sys.path.append('/root/autodl-tmp/Text2midi/model')
from data_loader_remi import Text2MusicDataset

# ‚úÖ ÂØºÂÖ•Ê®°ÂûãÔºàH800ÂêåÊ†∑ÊîØÊåÅFlash Attention 2Ôºâ
from model.transformer_model_RoPE_Flash2_KVCache_RTX5090 import Transformer, FLASH_ATTN_AVAILABLE

from torch.utils.data import DataLoader

logger = get_logger(__name__)

# ==================== ÂÖ®Â±ÄÂ∏∏Èáè ====================
PAD_ID = 0  # REMI tokenizerÁöÑpadding ID


# ==================== ÈÖçÁΩÆÂèÇÊï∞ ====================
class H800FixedTrainingConfig:
    """‰øÆÂ§çÁâàËÆ≠ÁªÉÈÖçÁΩÆ - H800 80GB‰ºòÂåñ"""
    def __init__(self):
        # Ë∑ØÂæÑÈÖçÁΩÆ
        self.config_file = "configs/config.yaml"
        self.pretrained_model_path = None
        self.vocab_path = "artifacts/vocab_remi.pkl"

        self.caption_dataset_path = "/root/autodl-tmp/Text2midi/captions/captionMidiCapsPlus.json"
        self.midi_folder_path = r"/root/autodl-tmp/Text2midi/MidiCaps"
        self.output_dir = "/root/autodl-tmp/Text2midi/output_MidiCaps_H800_FIXED"

        # Ê®°ÂûãÈÖçÁΩÆ
        self.decoder_d_model = 768
        self.decoder_num_heads = 12
        self.decoder_num_layers = 18

        # üöÄ H800‰ºòÂåñÔºöÂ¢ûÂ§ßÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶Ôºà80GBÊòæÂ≠òÔºâ
        self.decoder_max_sequence_length = 15000  # ‚¨ÜÔ∏è ‰ªé15000ÊèêÈ´òÂà∞15000
        self.filter_long_samples = True

        self.decoder_intermediate_size = 3072
        self.use_moe = False
        self.num_experts = 4

        # üîß ‰øÆÂ§ç2ÔºöÂ¢ûÂä†ËÆ≠ÁªÉÈáèÔºåÊèêÈ´òÂ≠¶‰π†Áéá
        self.epochs = 30  # ‚¨ÜÔ∏è ‰ªé10ÊèêÈ´òÂà∞20
        self.learning_rate = 1e-5  # ‚¨ÜÔ∏è ‰ªé5e-6ÊèêÈ´òÂà∞1e-5

        # üöÄ H800‰ºòÂåñÔºöÂ¢ûÂ§ßbatch sizeÔºåÂáèÂ∞ëgradient accumulationÔºàÊ¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºâ
        self.per_device_train_batch_size = 6  # ÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÂêéÂèØÁî®6
        self.gradient_accumulation_steps = 11  # ‰øùÊåÅÊúâÊïàbatch=66

        # üöÄ H800‰ºòÂåñÔºöË∞ÉÊï¥warmupÂíå‰øùÂ≠òÈ¢ëÁéá
        self.num_warmup_steps = 15000
        self.lr_scheduler_type = "cosine"
        self.save_every = 5  # ÊØè2‰∏™epoch‰øùÂ≠ò

        self.max_train_steps = None

        # KV CacheÈÖçÁΩÆ
        self.max_cache_len = 15000  # ‚¨ÜÔ∏è ÂåπÈÖçmax_sequence_length

        # ‚ö° ‰ºòÂåñ5ÔºöÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºàËäÇÁúÅÊòæÂ≠ò30-40%Ôºâ
        self.use_gradient_checkpointing = True  # ÂÖÅËÆ∏Êõ¥Â§ßbatch size

        # ‚ö° ‰ºòÂåñ6ÔºöÂêØÁî®torch.compileÂä†ÈÄü
        self.use_torch_compile = False  # PyTorch 2.7Êñ∞ÁâπÊÄß

        # ÂÆûÈ™åËøΩË∏™ÈÖçÁΩÆ
        self.with_tracking = True
        self.report_to = "wandb"
        self.project_name = "MidiCaps-FIXED-H800-80GB"

        # Âä†ËΩΩyamlÈÖçÁΩÆ
        self.load_yaml_config()

    def load_yaml_config(self):
        try:
            with open(self.config_file, 'r') as f:
                yaml_config = yaml.safe_load(f)
                self.artifact_folder = yaml_config.get('artifact_folder', 'artifacts')
        except Exception as e:
            print(f"Warning: Could not load yaml config: {e}")
            self.artifact_folder = 'artifacts'


def collate_fn(batch):
    """
    Êï∞ÊçÆÊâπÂ§ÑÁêÜÂáΩÊï∞

    Ê≥®ÊÑèÔºöpadding‰ΩøÁî®PAD_ID=0Ôºå‰ΩÜ‰ºöÂú®lossËÆ°ÁÆóÊó∂Ë¢´ignore
    """
    input_ids = [item[0].squeeze(0) for item in batch]
    attention_mask = [item[1].squeeze(0) for item in batch]
    labels = [item[2] if item[2].dim() == 1 else item[2].squeeze(0) for item in batch]

    # Padding with PAD_ID=0
    input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=PAD_ID)
    attention_mask = nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=PAD_ID)

    return input_ids, attention_mask, labels


def setup_training(config):
    """ËÆæÁΩÆËÆ≠ÁªÉÁéØÂ¢É"""
    accelerator_log_kwargs = {}
    if config.with_tracking:
        accelerator_log_kwargs["log_with"] = config.report_to
        accelerator_log_kwargs["project_dir"] = config.output_dir

    # ‚úÖ H800: BF16Ê∑∑ÂêàÁ≤æÂ∫¶ÔºàHopperÊû∂ÊûÑÂéüÁîüÊîØÊåÅÔºâ
    accelerator = Accelerator(
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        mixed_precision='bf16',
        **accelerator_log_kwargs
    )

    # ÈÖçÁΩÆÊó•Âøó
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger.info(accelerator.state, main_process_only=False)

    if accelerator.is_main_process:
        logger.info("="*70)
        logger.info("üöÄ FIXED Training Configuration - H800 80GB")
        logger.info("="*70)
        logger.info(f"GPU: H800 (80GB) x1 - HopperÊû∂ÊûÑ")
        logger.info(f"Max Seq Len: {config.decoder_max_sequence_length} (‚¨ÜÔ∏è vs RTX5090: 15000)")
        logger.info(f"Batch Size: {config.per_device_train_batch_size} (‚¨ÜÔ∏è vs RTX5090: 2)")
        logger.info(f"Gradient Acc: {config.gradient_accumulation_steps} (‚¨áÔ∏è vs RTX5090: 32)")
        logger.info(f"Effective Batch: {config.per_device_train_batch_size * config.gradient_accumulation_steps}")
        logger.info(f"")
        logger.info(f"üîß KEY FIXES:")
        logger.info(f"  1. Padding Mask: ignore_index={PAD_ID} ‚úÖ")
        logger.info(f"  2. Label Smoothing: 0 (was 0.1) ‚úÖ")
        logger.info(f"  3. Epochs: {config.epochs} (was 10) ‚úÖ")
        logger.info(f"  4. Learning Rate: {config.learning_rate} (was 5e-6) ‚úÖ")
        logger.info(f"")
        logger.info(f"üöÄ H800 OPTIMIZATIONS:")
        logger.info(f"  1. Batch Size: 2‚Üí8 (4x larger)")
        logger.info(f"  2. Gradient Acc: 32‚Üí8 (4x less)")
        logger.info(f"  3. Max Seq Len: 15000‚Üí15000 (33% longer)")
        logger.info(f"  4. DataLoader workers: 8‚Üí16 (20Ê†∏CPU)")
        logger.info(f"")

        if FLASH_ATTN_AVAILABLE:
            logger.info("‚úÖ Flash Attention 2 ENABLED (Hopper‰ºòÂåñ)")
        else:
            logger.warning("‚ö†Ô∏è Flash Attention NOT available")

        logger.info("="*70)

    # ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
    if accelerator.is_main_process:
        if config.output_dir is None or config.output_dir == "":
            config.output_dir = "saved/" + str(int(time.time()))
        os.makedirs(config.output_dir, exist_ok=True)
        os.makedirs(f"{config.output_dir}/checkpoints", exist_ok=True)

        # ÂàùÂßãÂåñwandb
        if config.with_tracking:
            wandb.login()
            wandb.init(
                project=config.project_name,
                name=f"fixed-h800-{time.strftime('%Y%m%d-%H%M%S')}",
                settings=wandb.Settings(init_timeout=120),
                config={
                    "gpu": "H800 (80GB) Hopper",
                    "learning_rate": config.learning_rate,
                    "epochs": config.epochs,
                    "batch_size": config.per_device_train_batch_size,
                    "gradient_accumulation_steps": config.gradient_accumulation_steps,
                    "max_seq_len": config.decoder_max_sequence_length,
                    "optimization": "H800 + FIXED + Flash Attention 2 + BF16",
                    "fixes": "padding_mask + label_smoothing + training_amount",
                    "label_smoothing": 0,
                    "ignore_padding": True,
                    "h800_optimizations": "8x_batch + 20k_seq + 16workers"
                }
            )

    accelerator.wait_for_everyone()
    return accelerator


def load_dataset(config, tokenizer, accelerator):
    """üöÄ H800‰ºòÂåñÁöÑÊï∞ÊçÆÂä†ËΩΩÔºà16 workersÔºå20Ê†∏CPUÔºâ"""
    logger.info("Loading dataset...")

    with jsonlines.open(config.caption_dataset_path) as reader:
        captions = list(reader)

    logger.info(f"Loaded {len(captions)} captions")

    # ‚ö° ËøáÊª§Ë∂ÖÈïøÊ†∑Êú¨
    if config.filter_long_samples:
        logger.info(f"Filter enabled for samples > {config.decoder_max_sequence_length} tokens")

    temp_config = {
        'raw_data': {
            'raw_data_folders': {
                'MidiCaps': {
                    'folder_path': config.midi_folder_path,
                    'file_extension': 'mid'
                }
            }
        },
        'model': {
            'text2midi_model': {
                'decoder_max_sequence_length': config.decoder_max_sequence_length
            }
        },
        'artifact_folder': config.artifact_folder
    }

    with accelerator.main_process_first():
        dataset = Text2MusicDataset(
            temp_config,
            captions,
            remi_tokenizer=tokenizer,
            mode="train",
            shuffle=True
        )

        # üöÄ H800‰ºòÂåñÔºöÂ¢ûÂä†workersÂà∞16Ôºà20Ê†∏CPUÔºâ
        dataloader = DataLoader(
            dataset,
            batch_size=config.per_device_train_batch_size,
            shuffle=True,
            num_workers=8,  # ‚¨ÜÔ∏è ‰ªé8ÊèêÈ´òÂà∞16
            collate_fn=collate_fn,
            drop_last=True,
            pin_memory=True,
            prefetch_factor=8,#4
            persistent_workers=True
        )

    logger.info(f"Dataset size: {len(dataset)}")
    logger.info(f"DataLoader: workers=16, prefetch=4 (H800‰ºòÂåñ)")
    return dataset, dataloader


def create_or_load_model(config, vocab_size, device):
    """ÂàõÂª∫ÊàñÂä†ËΩΩÊ®°Âûã"""
    logger.info("Creating model...")

    model = Transformer(
        n_vocab=vocab_size,
        d_model=config.decoder_d_model,
        nhead=config.decoder_num_heads,
        max_len=config.decoder_max_sequence_length,
        num_decoder_layers=config.decoder_num_layers,
        dim_feedforward=config.decoder_intermediate_size,
        use_moe=config.use_moe,
        num_experts=config.num_experts,
        max_cache_len=config.max_cache_len,
        device=device
    )

    # Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊùÉÈáç
    if config.pretrained_model_path and os.path.exists(config.pretrained_model_path):
        logger.info(f"Loading pretrained model: {config.pretrained_model_path}")
        try:
            state_dict = torch.load(config.pretrained_model_path, map_location=device)
            model.load_state_dict(state_dict, strict=False)
            logger.info("Successfully loaded pretrained model!")
        except Exception as e:
            logger.warning(f"Failed to load: {e}")
    else:
        logger.info("Training from scratch")

    # üîß ÂÖ≥ÈîÆ‰øÆÂ§çÔºöÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÊù•ËäÇÁúÅÊòæÂ≠ò
    if config.use_gradient_checkpointing:
        logger.info("="*70)
        logger.info("üîß ÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ (Gradient Checkpointing)")
        logger.info("="*70)
        logger.info("È¢ÑÊúüÊïàÊûúÔºö")
        logger.info("  - ÊòæÂ≠òËäÇÁúÅ: ~30-40% ‚úÖ")
        logger.info("  - ÈÄüÂ∫¶ÂΩ±Âìç: ~10-15%ÊÖ¢ (ÂèØÊé•Âèó)")
        logger.info("  - ÂÖÅËÆ∏batch size: 6-8 (vs ÂΩìÂâç4)")
        logger.info("="*70)

        # ÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÔºö‰øÆÊîπdecoderÁöÑforwardÊñπÊ≥ïÊù•‰ΩøÁî®checkpointing
        model.decoder.gradient_checkpointing = True
        logger.info(f"‚úÖ Â∑≤‰∏∫ {len(model.decoder.layers)} ‰∏™decoderÂ±ÇÂêØÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ")

    # ‚ö° ‰ºòÂåñÔºö‰ΩøÁî®torch.compileÂä†ÈÄü
    if config.use_torch_compile:
        try:
            logger.info("‚ö° Compiling model with torch.compile...")
            model = torch.compile(model, mode="reduce-overhead")
            logger.info("‚úÖ Model compiled successfully!")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è torch.compile failed: {e}")
            logger.warning("Continuing without compilation...")

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"Total trainable parameters: {total_params:,}")

    return model


def train_model(config, model, dataloader, accelerator, device):
    """üîß ‰øÆÂ§çÁâàËÆ≠ÁªÉÂæ™ÁéØ - H800‰ºòÂåñ"""
    model.train()
    # ===== Ê∑ªÂä†ËøôÊÆµÈ™åËØÅ‰ª£Á†Å =====
    print("\n" + "="*70)
    print("üîç È™åËØÅFlash AttentionÁä∂ÊÄÅ")
    print("="*70)
    print(f"Ê®°ÂûãtrainingÊ®°Âºè: {model.training}")
    print(f"Flash AttentionÂèØÁî®:{model.decoder.layers[0].self_attn.forward.__code__.co_filename}")
    # ÊµãËØï‰∏ÄÊ¨°forwardÁúãËµ∞Âì™‰∏™ÂàÜÊîØ
    test_input = torch.randn(2, 100, 768, device=device)
    layer = model.decoder.layers[0].self_attn
    print(f"\nÊµãËØïattention forward:")
    print(f"  - layer.training = {layer.training}")
    print(f"  - Â∫îËØ•‰ΩøÁî® Flash Attention: {layer.training}")
    # ===== È™åËØÅ‰ª£Á†ÅÁªìÊùü =====
    
    logger.info("Starting FIXED training on H800...")

    # ËÆæÁΩÆ‰ºòÂåñÂô®
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        betas=(0.9, 0.999),
        weight_decay=0.05
    )

    # ËÆ°ÁÆóËÆ≠ÁªÉÊ≠•Êï∞
    num_update_steps_per_epoch = math.ceil(
        len(dataloader) / config.gradient_accumulation_steps
    )

    if config.max_train_steps is None:
        max_train_steps = config.epochs * num_update_steps_per_epoch
    else:
        max_train_steps = config.max_train_steps
        config.epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)

    # Â≠¶‰π†ÁéáË∞ÉÂ∫¶Âô®
    lr_scheduler = get_scheduler(
        name=config.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=int(0.10 * max_train_steps),  # 10% warmup
        num_training_steps=max_train_steps,
    )

    # Prepare with accelerator
    model, optimizer, lr_scheduler, dataloader = accelerator.prepare(
        model, optimizer, lr_scheduler, dataloader
    )

    # ËÆ≠ÁªÉ‰ø°ÊÅØ
    total_batch_size = config.per_device_train_batch_size * config.gradient_accumulation_steps

    logger.info("***** H800 FIXED Training Configuration *****")
    logger.info(f"  Num Epochs = {config.epochs}")
    logger.info(f"  Batch size per device = {config.per_device_train_batch_size}")
    logger.info(f"  Gradient Accumulation = {config.gradient_accumulation_steps}")
    logger.info(f"  Effective batch size = {total_batch_size}")
    logger.info(f"  Total optimization steps = {max_train_steps}")
    logger.info(f"  Max Seq Length = {config.decoder_max_sequence_length}")
    logger.info(f"  Learning Rate = {config.learning_rate}")
    logger.info(f"  Label Smoothing = 0 (FIXED)")
    logger.info(f"  Ignore Padding = True (FIXED)")

    # üöÄ H800È¢Ñ‰º∞ËÆ≠ÁªÉÊó∂Èó¥ÔºàÊ¢ØÂ∫¶Ê£ÄÊü•ÁÇπ + batch=6Ôºâ
    # Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπÂ¢ûÂä†15% overheadÔºå‰ΩÜÂáèÂ∞ëÁ¥ØÁßØÊ≠•Êï∞Ë°•ÂÅø
    estimated_time_per_step = 11  # batch=6, grad_acc=11, Á∫¶11Áßí/Ê≠•
    total_hours = (max_train_steps * estimated_time_per_step) / 3600
    logger.info(f"  Estimated training time: {total_hours:.1f} hours ({total_hours/24:.1f} days)")
    logger.info(f"  Estimated time/step: {estimated_time_per_step}s (with gradient checkpointing)")
    logger.info(f"  (H800: ~{total_hours/24:.1f} days vs RTX5090: ~{total_hours/24*1.5:.1f} days)")

    # üîß ‰øÆÂ§ç1: ÊçüÂ§±ÂáΩÊï∞ - Ê∑ªÂä†ignore_index=PAD_ID
    # üîß ‰øÆÂ§ç2: Label SmoothingËÆæ‰∏∫0
    criterion = nn.CrossEntropyLoss(
        label_smoothing=0,  # üîß ‰ªé0.1Êîπ‰∏∫0
        ignore_index=PAD_ID  # üîß ÂÖ≥ÈîÆ‰øÆÂ§çÔºöÂøΩÁï•paddingÁöÑ0
    )

    logger.info(f"‚úÖ Loss function configured:")
    logger.info(f"   - ignore_index={PAD_ID} (padding will not contribute to loss)")
    logger.info(f"   - label_smoothing=0 (disabled for better convergence)")

    # ËÆ≠ÁªÉÂæ™ÁéØ
    progress_bar = tqdm(
        range(max_train_steps),
        disable=not accelerator.is_local_main_process
    )
    completed_steps = 0
    starting_epoch = 0
    best_loss = float('inf')

    model.train()

    # ÊòæÂ≠òÁõëÊéß
    if accelerator.is_main_process and torch.cuda.is_available():
        logger.info(f"Initial GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB / 80 GB")

    # ‚ö° ËÆ≠ÁªÉÂºÄÂßãÊó∂Èó¥
    training_start_time = time.time()
    step_times = []

    for epoch in range(starting_epoch, config.epochs):
        total_loss = 0
        epoch_loss = 0
        epoch_start_time = time.time()

        for step, batch in enumerate(dataloader):
            step_start_time = time.time()

            with accelerator.accumulate(model):
                encoder_input, attention_mask, tgt = batch

                encoder_input = encoder_input.to(device)
                attention_mask = attention_mask.to(device)
                tgt = tgt.to(device)

                # ÂáÜÂ§ádecoderËæìÂÖ•ÂíåÁõÆÊ†á
                tgt_input = tgt[:, :-1]
                tgt_output = tgt[:, 1:]

                # ÂâçÂêë‰º†Êí≠
                if config.use_moe:
                    outputs, aux_loss = model(encoder_input, attention_mask, tgt_input)
                else:
                    outputs = model(encoder_input, attention_mask, tgt_input)
                    aux_loss = 0

                # üîß ËÆ°ÁÆóÊçüÂ§±ÔºàpaddingÂ∑≤Ë¢´Ëá™Âä®ÂøΩÁï•Ôºâ
                loss = criterion(
                    outputs.view(-1, outputs.size(-1)),
                    tgt_output.reshape(-1)
                )
                loss += aux_loss

                total_loss += loss.detach().float()
                epoch_loss += loss.detach().float()

                # ÂèçÂêë‰º†Êí≠
                accelerator.backward(loss)

                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            # Êõ¥Êñ∞ËøõÂ∫¶Êù°
            if accelerator.sync_gradients:
                step_time = time.time() - step_start_time
                step_times.append(step_time)

                # ËÆ°ÁÆóÂπ≥ÂùáÊ≠•Êó∂Èó¥ÂíåÂâ©‰ΩôÊó∂Èó¥
                if len(step_times) > 10:
                    avg_step_time = np.mean(step_times[-100:])
                    remaining_steps = max_train_steps - completed_steps
                    eta_seconds = remaining_steps * avg_step_time
                    eta_hours = eta_seconds / 3600

                    progress_bar.set_postfix({
                        "epoch": epoch + 1,
                        "loss": f"{loss.item():.4f}",
                        "lr": f"{lr_scheduler.get_last_lr()[0]:.2e}",
                        "step_time": f"{step_time:.1f}s",
                        "eta": f"{eta_hours:.1f}h"
                    })
                else:
                    progress_bar.set_postfix({
                        "epoch": epoch + 1,
                        "loss": f"{loss.item():.4f}",
                        "lr": f"{lr_scheduler.get_last_lr()[0]:.2e}",
                        "step_time": f"{step_time:.1f}s"
                    })

                progress_bar.update(1)
                completed_steps += 1

                # ËÆ∞ÂΩïÂà∞wandb
                if accelerator.is_main_process and config.with_tracking:
                    log_dict = {
                        "train_loss": loss.item(),
                        "learning_rate": lr_scheduler.get_last_lr()[0],
                        "epoch": epoch + 1,
                        "step": completed_steps,
                        "step_time": step_time,
                        "sequence_length": tgt_input.size(1)
                    }

                    if torch.cuda.is_available():
                        log_dict["gpu_memory_gb"] = torch.cuda.memory_allocated()/1e9

                    wandb.log(log_dict)

            if completed_steps >= max_train_steps:
                break

        # EpochÁªìÊùü
        epoch_time = time.time() - epoch_start_time
        avg_epoch_loss = epoch_loss.item() / len(dataloader)

        if accelerator.is_main_process:
            logger.info(f"\nEpoch {epoch+1}/{config.epochs} completed in {epoch_time/60:.1f} min")
            logger.info(f"Avg Loss: {avg_epoch_loss:.4f}")

            # ÊòæÂ≠òÁªüËÆ°
            if torch.cuda.is_available():
                current_mem = torch.cuda.memory_allocated()/1e9
                peak_mem = torch.cuda.max_memory_allocated()/1e9
                logger.info(f"GPU Memory: {current_mem:.2f} GB / {peak_mem:.2f} GB (peak) / 80 GB total")
                torch.cuda.reset_peak_memory_stats()

            # ‰øùÂ≠òcheckpoint
            if (epoch + 1) % config.save_every == 0:
                checkpoint_dir = f"{config.output_dir}/checkpoints/epoch_{epoch+1}"
                os.makedirs(checkpoint_dir, exist_ok=True)

                unwrapped_model = accelerator.unwrap_model(model)
                torch.save(
                    unwrapped_model.state_dict(),
                    f"{checkpoint_dir}/pytorch_model.bin"
                )
                logger.info(f"Checkpoint saved to {checkpoint_dir}")

            # ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã
            if avg_epoch_loss < best_loss:
                best_loss = avg_epoch_loss
                best_model_dir = f"{config.output_dir}/best_model"
                os.makedirs(best_model_dir, exist_ok=True)

                unwrapped_model = accelerator.unwrap_model(model)
                torch.save(
                    unwrapped_model.state_dict(),
                    f"{best_model_dir}/pytorch_model.bin"
                )
                logger.info(f"Best model saved (loss: {best_loss:.4f})")

        accelerator.wait_for_everyone()

    # ËÆ≠ÁªÉÂÆåÊàê
    total_training_time = time.time() - training_start_time

    if accelerator.is_main_process:
        logger.info("\n" + "="*70)
        logger.info("üöÄ H800 FIXED Training Completed!")
        logger.info(f"Total time: {total_training_time/3600:.2f} hours ({total_training_time/86400:.2f} days)")
        logger.info(f"Best loss: {best_loss:.4f}")
        logger.info("="*70)

        final_model_dir = f"{config.output_dir}/final_model"
        os.makedirs(final_model_dir, exist_ok=True)

        unwrapped_model = accelerator.unwrap_model(model)
        torch.save(
            unwrapped_model.state_dict(),
            f"{final_model_dir}/pytorch_model.bin"
        )
        logger.info(f"Final model saved to {final_model_dir}")

        if config.with_tracking:
            wandb.finish()


def main():
    """‰∏ªÂáΩÊï∞ - H800 ‰øÆÂ§çÁâà"""
    os.environ['WANDB_MODE'] = 'offline'

    parser = argparse.ArgumentParser(
        description="üöÄ H800 FIXED Training with Flash Attention 2 (80GB)"
    )
    parser.add_argument("--pretrained_model", type=str, default=None)
    parser.add_argument("--caption_path", type=str)
    parser.add_argument("--midi_folder", type=str)
    parser.add_argument("--output_dir", type=str, default="output_H800_FIXED")
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=6)  # H800 with gradient checkpointing
    parser.add_argument("--gradient_accumulation", type=int, default=11)  # ‰øùÊåÅÊúâÊïàbatch=66
    parser.add_argument("--learning_rate", type=float, default=5e-6)
    parser.add_argument("--max_seq_len", type=int, default=15000)  # H800ÈªòËÆ§15000
    parser.add_argument("--use_gradient_checkpointing", action="store_true", default=True)  # ÈªòËÆ§ÂêØÁî®
    args = parser.parse_args()

    # ÂàõÂª∫ÈÖçÁΩÆ
    config = H800FixedTrainingConfig()

    # ‰ªéÂëΩ‰ª§Ë°åÂèÇÊï∞Êõ¥Êñ∞ÈÖçÁΩÆ
    if args.pretrained_model:
        config.pretrained_model_path = args.pretrained_model
    config.caption_dataset_path = args.caption_path
    print("\n" + "="*70)
    print(f"caption_dataset_path: {config.caption_dataset_path}")
    config.midi_folder_path = args.midi_folder
    print(f"midi_folder_path: {config.midi_folder_path}")
    config.output_dir = args.output_dir
    print(f"output_dir: {config.output_dir}")
    config.epochs = args.epochs
    config.per_device_train_batch_size = args.batch_size
    config.gradient_accumulation_steps = args.gradient_accumulation
    config.learning_rate = args.learning_rate
    config.decoder_max_sequence_length = args.max_seq_len
    config.use_gradient_checkpointing = args.use_gradient_checkpointing

    print("\n" + "="*70)
    print("üöÄ H800 FIXED Training Configuration (80GB)")
    print("="*70)
    print(f"\nüîß KEY FIXES:")
    print(f"  1. Padding Mask: ignore_index={PAD_ID} ‚úÖ")
    print(f"  2. Label Smoothing: 0 (was 0.1) ‚úÖ")
    print(f"  3. Epochs: {config.epochs} (was 10) ‚úÖ")
    print(f"  4. Learning Rate: {config.learning_rate} (was 5e-6) ‚úÖ")
    print(f"  5. Gradient Checkpointing: {'ÂêØÁî®' if config.use_gradient_checkpointing else 'Á¶ÅÁî®'} ‚úÖ")
    print(f"\nüöÄ H800 OPTIMIZATIONS:")
    print(f"  Batch Size: {config.per_device_train_batch_size} (vs RTX5090: 2)")
    print(f"  Gradient Acc: {config.gradient_accumulation_steps} (vs RTX5090: 32)")
    print(f"  Effective Batch: {config.per_device_train_batch_size * config.gradient_accumulation_steps}")
    print(f"  Max Seq Len: {config.decoder_max_sequence_length} (vs RTX5090: 15000)")
    print(f"  DataLoader: workers=8, prefetch=8 (vs RTX5090: 8workers)")
    print(f"  Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ: {'ÂêØÁî®(ËäÇÁúÅ30-40%ÊòæÂ≠ò)' if config.use_gradient_checkpointing else 'Á¶ÅÁî®'}")
    print("="*70)

    # ËÆ°ÁÆó‰º∞ÁÆó
    total_steps = (168385 / config.per_device_train_batch_size) * config.epochs / config.gradient_accumulation_steps

    # Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ‰ºöÂ¢ûÂä†~15%ËÆ°ÁÆóÊó∂Èó¥Ôºå‰ΩÜÂáèÂ∞ëÊ¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞ÂèØ‰ª•ÊäµÊ∂à
    # batch=6, grad_acc=11: ÊØè‰∏™stepÈúÄË¶Å11Ê¨°forward/backward
    # È¢Ñ‰º∞: 0.8s GPUËÆ°ÁÆó + 11√ó0.9s Á¥ØÁßØ = ~10.7s/step (ËÄÉËôëÊ¢ØÂ∫¶Ê£ÄÊü•ÁÇπoverhead)
    estimated_time_per_step = 11  # Áßí
    estimated_hours = (total_steps * estimated_time_per_step) / 3600
    estimated_days = estimated_hours / 24

    rtx5090_steps = (168385 / 2) * 20 / 32
    rtx5090_hours = (rtx5090_steps * 15) / 3600
    rtx5090_days = rtx5090_hours / 24

    print(f"\n‚è±Ô∏è Estimated Training Time:")
    print(f"  Total steps: {total_steps:,.0f}")
    print(f"  Estimated time/step: {estimated_time_per_step}s (with gradient checkpointing)")
    print(f"  H800: ~{estimated_days:.1f} days ({estimated_hours:.0f} hours)")
    print(f"  RTX5090 (ÂØπÊØî): ~{rtx5090_days:.1f} days ({rtx5090_hours:.0f} hours)")
    print(f"  Speed-up: {rtx5090_days/estimated_days:.1f}x faster ‚ö°")
    print("="*70 + "\n")

    # ËÆæÁΩÆËÆ≠ÁªÉÁéØÂ¢É
    accelerator = setup_training(config)
    device = accelerator.device

    # Âä†ËΩΩtokenizer
    logger.info(f"Loading tokenizer from: {config.vocab_path}")
    with open(config.vocab_path, "rb") as f:
        tokenizer = pickle.load(f)
    vocab_size = len(tokenizer)
    logger.info(f"Vocab size: {vocab_size}")

    # Âä†ËΩΩÊï∞ÊçÆÈõÜ
    dataset, dataloader = load_dataset(config, tokenizer, accelerator)

    # ÂàõÂª∫ÊàñÂä†ËΩΩÊ®°Âûã
    model = create_or_load_model(config, vocab_size, device)

    # ÂºÄÂßãËÆ≠ÁªÉ
    train_model(config, model, dataloader, accelerator, device)


if __name__ == "__main__":
    main()
