# HID-MuseFormer 研发日记 - 2025年11月30日

## 今日核心工作：FlexAttention 优化与代码归档

### 1. FlexAttention 实现完成

将 FC-Attention 的自定义稀疏掩码迁移到 PyTorch 2.5+ 的 FlexAttention API，实现了**自定义 mask + Flash kernel 性能**的完美结合。

#### 技术方案

**核心创新：`mask_mod` 闭包函数**

```python
def mask_mod(b, h, q_idx, kv_idx):
    # 1. 因果性：只能看过去
    causal = q_idx >= kv_idx

    # 2. 全局 token：所有 token 都能看到 (BOS, BPM, TS, 乐器标记, SEP)
    is_global_k = (k_chord == -1) | (k_inst == 129)

    # 3. 同乐器：全连接（相似度高，主题回顾需要）
    same_inst = (q_inst == k_inst) & (q_inst < 129)

    # 4. 跨乐器近距离 (offset <= 2)：全连接（和声协调）
    cross_near = diff_inst & (chord_diff >= 0) & (chord_diff <= 2)

    # 5. 跨乐器远距离：只看 offset=4（乐段边界）
    cross_far = diff_inst & (chord_diff == 4)

    return causal & (is_global_k | same_inst | cross_near | cross_far)
```

**FlexAttention 优势**

| 对比项 | FC-Attention (旧) | Flash Attention | FlexAttention (新) |
|--------|-------------------|-----------------|-------------------|
| 自定义 mask | ✅ 支持 | ❌ 不支持 | ✅ 支持 |
| Flash kernel | ❌ fallback | ✅ 原生 | ✅ 原生 |
| 跨乐器连接设计 | ✅ 精确 | ❌ 只通过序列顺序 | ✅ 精确 |
| 显存 (batch=4, seq=24K) | OOM | ~25 GB | ~30 GB |
| Block 级稀疏优化 | ❌ | ❌ | ✅ 跳过全 0 的 128×128 block |

#### 关键实现细节

1. **RoPE 内联**：将 `RotaryPositionEmbedding` 代码直接内联到 `attention_flex.py`，避免依赖已归档文件

2. **越界索引处理**：FlexAttention 的 `create_block_mask` 会将序列长度向上取整到 128，需要用 `clamp` 安全处理

3. **编译优化**：使用 `_compile=True` 加速 BlockMask 创建

4. **Head 共享**：设置 `H=None` 让所有 head 共享同一 mask，减少内存

---

### 2. 代码归档与简化

将旧版本 attention 模块归档，保留唯一的 FlexAttention 实现：

#### 归档前
```
model/
├── attention.py          # FC-Attention (完整掩码，慢)
├── attention_flash.py    # Flash Attention (纯 causal，快但无自定义)
├── attention_flex.py     # FlexAttention (新)
└── hid_museformer.py     # 多模式切换逻辑
```

#### 归档后
```
model/
├── attention_flex.py     # FlexAttention (唯一，自包含 RoPE)
└── hid_museformer.py     # 简化，只使用 FlexAttention

archived/attention_legacy/
├── attention.py          # 已归档
└── attention_flash.py    # 已归档
```

#### 修改内容

1. **attention_flex.py**
   - 内联 RoPE 实现（`RotaryPositionEmbedding`, `rotate_half`, `apply_rotary_pos_emb`）
   - 移除对 `attention.py` 的依赖

2. **hid_museformer.py**
   - 移除 `attention_mode` 参数
   - 移除多模式切换逻辑
   - 只使用 `FlexFCAttentionBlock` 和 `FlexFCAttentionMask`

3. **train_h800.py**
   - 移除 `--attention_mode` 命令行参数
   - 统一使用 FlexAttention

---

### 3. FC-Attention 掩码规则回顾

基于 450K MIDI 文件的跨乐器相似度统计分析设计：

| 规则 | 说明 | 依据 |
|------|------|------|
| 同乐器全连接 | 即使 offset=29 相似度 (0.389) 仍高于跨乐器 | 主题回顾、动机发展 |
| 跨乐器 offset≤2 全连接 | 2 小节内和声协调 | 实时配合需要 |
| 跨乐器 offset=4 | 只看 4 小节前 | 乐段边界参考 |
| 全局 token 可见 | BOS, BPM, TS, 乐器标记, SEP | 结构信息 |
| 因果性 | 只能看过去 | 自回归生成 |

---

### 4. 后台任务状态

- **数据预处理**: 450K 文件预处理中（预计 ~25 小时）
- **MIDI 数据集压缩**: 进行中
- **跨乐器相似度分析**: 运行中

---

### 5. 下一步计划

1. **H800 服务器部署**
   - 确认 PyTorch 版本 >= 2.5
   - 测试 FlexAttention 在 H800 80GB 上的实际性能

2. **训练启动**
   - 使用预处理完成的数据
   - 监控显存使用和训练速度

3. **性能基准测试**
   - 对比 FlexAttention vs 纯 causal Flash Attention
   - 记录不同 batch size 和 seq_len 的显存占用

---

### 6. Token 类型级稀疏实现

基于 11/29 的 NMI 分析，在 FlexAttention 中实现了 Token 类型级稀疏掩码：

#### TOKEN_TYPE_VISIBILITY 矩阵

```python
# 行 = Query, 列 = Key, 类型: 0=T, 1=P, 2=D, 3=V
TOKEN_TYPE_VISIBILITY = [
    # Key:  T      P      D      V
    [True,  True,  False, False],  # Query T: 看 T, P
    [True,  True,  False, False],  # Query P: 看 T, P
    [False, False, False, False],  # Query D: 不看任何
    [False, False, False, True ],  # Query V: 只看 V
]
```

#### 设计依据

| Token 类型组合 | NMI 值 | 处理方式 |
|---------------|--------|---------|
| T→T | 0.193 | ✅ 可见 |
| P→P | 0.144 | ✅ 可见 |
| V→V | 0.152 | ✅ 可见 |
| T↔P | ~0.09 | ✅ 保留 |
| D 相关 | < 0.07 | ❌ 移除 |

#### 同音符规则

同一 `note_id` 的 T/P/D/V token 之间**全连接**（同一音符的 4 个属性高度相关）

---

### 7. Summary Token 机制实现

实现了 MuseFormer 核心创新：Summary Token 压缩 bar 信息供远距离 token 访问。

#### 四种 Attention Block

| Block | Query | Key/Value | 用途 |
|-------|-------|-----------|------|
| ss | Summary | Summary | 粗粒度跨 bar 交互 |
| sr | Summary | Regular | 信息压缩 (S 聚合同 bar 的 R) |
| rs | Regular | Summary (K2,V2) | 获取远距离上下文 |
| rr | Regular | Regular | 细粒度近距离交互 |

#### 信息流

```
阶段 1: Summarize
  SS: S_i attend to S_0..S_{i} (因果)
  SR: S_i attend to bar_i 的所有 R
  → sum_x2 = Softmax(Q_s × [K_s; K_r]) × [V_s; V_r]

阶段 2: 二次投影
  K2 = W_k2 × sum_x2
  V2 = W_v2 × sum_x2

阶段 3: Updating
  RS: R_j attend to S_0..S_{bar(R_j)-1} (只看已完成 bar)
  RR: 现有的 FC-Attention 逻辑
  → reg_output = Softmax(Q_r × [K2; K_r]) × [V2; V_r]
```

#### 新增文件

- **`attention_flex_summary.py`**: Summary Token FlexAttention 实现
  - `SummaryTokenEmbedding`: 可学习的 Summary 嵌入
  - `FlexSummaryAttentionMask`: ss, sr, rs, rr 四种掩码生成器
  - `FlexSummaryAttention`: 包含 K2, V2 二次投影的 attention
  - `FlexSummaryAttentionBlock`: 完整的 block

---

### 8. 模型集成更新

#### hid_museformer.py 修改

- 添加 `use_summary_token` 配置选项
- forward() 支持 `token_type_ids` 和 `note_ids` 参数
- 条件性使用 Summary Token 或纯 Regular 模式

#### preprocess_data.py 修改

- 输出数据新增 `token_type_ids` 和 `note_ids` 字段

---

## 技术笔记

### FlexAttention API 要点

```python
from torch.nn.attention.flex_attention import flex_attention, create_block_mask

# 1. 定义 mask_mod (返回 True = 可注意)
def mask_mod(b, h, q_idx, kv_idx) -> bool:
    return condition

# 2. 创建 BlockMask (128×128 block 粒度)
block_mask = create_block_mask(
    mask_mod,
    B=batch_size,
    H=None,  # 所有 head 共享
    Q_LEN=seq_len,
    KV_LEN=seq_len,
    device=device,
    _compile=True  # 关键：编译加速
)

# 3. 调用 flex_attention
output = flex_attention(query, key, value, block_mask=block_mask)
```

### 注意事项

1. **PyTorch 版本**：需要 2.5+，否则 `from torch.nn.attention.flex_attention` 会失败

2. **闭包 tensor**：`mask_mod` 闭包中捕获的 tensor 需要是 contiguous 的

3. **越界处理**：`create_block_mask` 会评估超出实际 seq_len 的索引，需要 `clamp`

---

## 文件变更总结

| 文件 | 操作 | 说明 |
|------|------|------|
| `model/attention.py` | 移动 | → `archived/attention_legacy/` |
| `model/attention_flash.py` | 移动 | → `archived/attention_legacy/` |
| `model/attention_flex.py` | 修改 | 内联 RoPE 代码 |
| `model/hid_museformer.py` | 修改 | 移除多模式，只用 FlexAttention |
| `train_h800.py` | 修改 | 移除 `--attention_mode` 参数 |

---

*研发日记 by HID-MuseFormer Team*
