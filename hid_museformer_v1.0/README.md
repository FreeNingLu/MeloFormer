# MeloFormer v1.0

åŸºäº HID ç¼–ç çš„ç¬¦å·éŸ³ä¹ç”Ÿæˆæ¨¡å‹ï¼Œä½¿ç”¨ Summary Token + FlexAttention å®ç°é«˜æ•ˆç¨€ç–æ³¨æ„åŠ›ã€‚

**v1.0.1 ç‰¹æ€§**:
- âœ… ä¸‰é˜¶æ®µåŠ¨æ€ä¼˜åŒ– (batch_size + num_workers)
- âœ… æ™ºèƒ½æ˜¾å­˜ç®¡ç† (ç¼–è¯‘ 10GB â†’ è®­ç»ƒ 52GB)
- âœ… GPU åˆ©ç”¨ç‡ä¼˜åŒ– (ç¨³å®š 80-95%)

## ç¯å¢ƒè¦æ±‚

- Python 3.10+
- PyTorch 2.5+ (FlexAttention)
- CUDA 12.1+ (è®­ç»ƒ)

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### 1. è®­ç»ƒ (æ¨èé…ç½®)

#### RTX 6000 Ada (96GB) - æ¨è â­â­â­â­â­

```bash
python train.py \
    --model_size small \
    --data_dir ~/autodl-tmp/processed_data \
    --output_dir ~/autodl-tmp/checkpoints \
    --max_seq_len 8192 \
    --batch_size 8 \
    --gradient_accumulation_steps 6 \
    --learning_rate 3e-4 \
    --num_workers 8 \
    --epochs 3
```

**é¢„æœŸæ•ˆæœ**:
- è®­ç»ƒæ—¶é—´: ~50-52 å°æ—¶ (3 epochs)
- æˆæœ¬: ~Â¥260
- GPU åˆ©ç”¨ç‡: 80-95% (Phase 2 å)
- Tokens/s: ~10000 (Phase 3)

#### H800 80GB

```bash
python train.py \
    --model_size small \
    --data_dir ~/autodl-tmp/processed_data \
    --output_dir ~/autodl-tmp/checkpoints \
    --max_seq_len 8192 \
    --batch_size 6 \
    --gradient_accumulation_steps 8 \
    --learning_rate 3e-4 \
    --num_workers 8 \
    --epochs 3
```

**é¢„æœŸæ•ˆæœ**:
- è®­ç»ƒæ—¶é—´: ~40 å°æ—¶ (3 epochs)
- æˆæœ¬: ~Â¥355
- GPU åˆ©ç”¨ç‡: 95-100%
- Tokens/s: ~10000

### 2. ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹

è®­ç»ƒè¿‡ç¨‹è‡ªåŠ¨åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š

```
Phase 1 (Step 1-10): ç¼–è¯‘é˜¶æ®µ
â”œâ”€ batch_size=1, num_workers=0
â”œâ”€ GPU æ˜¾å­˜: ~10GB
â””â”€ æŒç»­æ—¶é—´: ~10 åˆ†é’Ÿ

Phase 2 (Step 11-100): æ•°æ®åŠ è½½ä¼˜åŒ–
â”œâ”€ batch_size=1, num_workers=8
â”œâ”€ GPU æ˜¾å­˜: ~10GB
â”œâ”€ GPU åˆ©ç”¨ç‡: 80-95% â¬†ï¸
â””â”€ æŒç»­æ—¶é—´: ~45-60 åˆ†é’Ÿ

Phase 3 (Step 101+): å…¨é€Ÿè®­ç»ƒ
â”œâ”€ batch_size=6-8, num_workers=8
â”œâ”€ GPU æ˜¾å­˜: 52-70GB
â”œâ”€ GPU åˆ©ç”¨ç‡: 95-100%
â””â”€ Tokens/s: 10000+
```

**æ—¥å¿—ç¤ºä¾‹**:

```
Step 10:
============================================================
ğŸ”„ Phase 2: åˆ‡æ¢æ•°æ®åŠ è½½
ğŸ‘· num_workers: 0 â†’ 8
ğŸ“Š batch_size: 1 (ä¿æŒ)
âš¡ é¢„æœŸ: GPU åˆ©ç”¨ç‡æå‡ï¼Œç¼–è¯‘ç»§ç»­...
============================================================

Step 100:
============================================================
ğŸš€ åˆ‡æ¢åˆ° Phase 3 (é«˜é€Ÿè®­ç»ƒæ¨¡å¼)
ğŸ“Š batch_size: 1 â†’ 6
ğŸ‘· num_workers: 8 â†’ 8
âš¡ é¢„æœŸåŠ é€Ÿ: 6.0x
============================================================
```

### 3. ç›‘æ§è®­ç»ƒ

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f train.log

# ç›‘æ§ GPU
watch -n 1 nvidia-smi

# æ£€æŸ¥è®­ç»ƒè¿›åº¦
grep "Phase\|Step.*Loss" train.log | tail -20
```

## ä¸»è¦å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `--model_size` | small | æ¨¡å‹å¤§å°: small/base/large/xlarge |
| `--max_seq_len` | 8192 | æœ€å¤§åºåˆ—é•¿åº¦ (æ¨è 8192) |
| `--batch_size` | 6-8 | ç›®æ ‡ batch (Phase 3 ä½¿ç”¨) |
| `--gradient_accumulation_steps` | 6-8 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `--learning_rate` | 3e-4 | å­¦ä¹ ç‡ |
| `--num_workers` | 8 | ç›®æ ‡ worker æ•° (Phase 2/3 ä½¿ç”¨) |
| `--epochs` | 3-5 | è®­ç»ƒè½®æ•° |

## æ¨¡å‹è§„æ ¼

| Size | Params | Layers | Dim | Heads | æ¨èæ˜¾å­˜ |
|------|--------|--------|-----|-------|---------|
| small | 17M | 6 | 256 | 4 | 80GB |
| base | 85M | 12 | 512 | 8 | 80GB |
| large | 200M | 16 | 768 | 12 | 80GB+ |
| xlarge | 450M | 24 | 1024 | 16 | 80GB+ |

## æ¶æ„

**Summary Token + FlexAttention**:
- SS: Summary â†’ Summary (ç²—ç²’åº¦è·¨ bar)
- SR: Summary â† Regular (ä¿¡æ¯å‹ç¼©)
- RS: Regular â†’ Summary (è¿œè·ç¦»ä¸Šä¸‹æ–‡)
- RR: Regular â†’ Regular (ç»†ç²’åº¦è¿‘è·ç¦»)

**ç¨€ç–ç­–ç•¥**:
- Bar çº§: åŒä¹å™¨å…¨è¿æ¥ï¼Œè·¨ä¹å™¨é€‰æ‹©æ€§
- Token ç±»å‹çº§: T-T, T-P, P-P, V-V å¯è§

## æ•…éšœæ’æŸ¥

### OOM é—®é¢˜

å¦‚æœé‡åˆ° OOM:

```bash
# æ–¹æ¡ˆ 1: é™ä½ batch_size
--batch_size 4  # æˆ–æ›´å°

# æ–¹æ¡ˆ 2: å¢åŠ æ¢¯åº¦ç´¯ç§¯
--gradient_accumulation_steps 16

# æ–¹æ¡ˆ 3: é™ä½åºåˆ—é•¿åº¦
--max_seq_len 4096
```

### GPU åˆ©ç”¨ç‡ä½

- ç¡®è®¤å·²ç»åˆ°è¾¾ Phase 2 (Step 11+)
- æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦æœ‰åˆ‡æ¢ä¿¡æ¯
- ç¡®è®¤ `num_workers` å·²åˆ‡æ¢åˆ° 8

### è®­ç»ƒé€Ÿåº¦æ…¢

- Phase 1-2 æ…¢æ˜¯æ­£å¸¸çš„ï¼ˆç¼–è¯‘ + ç¨³å®šæœŸï¼‰
- Phase 3 åº”è¾¾åˆ° 8000-10000 tokens/s
- å¦‚æœ Phase 3 ä»æ…¢ï¼Œæ£€æŸ¥ç£ç›˜ I/O

## ç‰ˆæœ¬ä¿¡æ¯

**å½“å‰ç‰ˆæœ¬**: v1.0.1
**å‘å¸ƒæ—¥æœŸ**: 2024-12-02

æŸ¥çœ‹å®Œæ•´æ›´æ–°æ—¥å¿—: [CHANGELOG.md](../CHANGELOG.md)

## æ€§èƒ½åŸºå‡†

### H800 80GB (å®æµ‹)

```
é…ç½®:
- small æ¨¡å‹ (17M)
- batch_size=6, seq_len=8192
- 529K æ ·æœ¬, 3 epochs

ç»“æœ:
- ç¼–è¯‘æ˜¾å­˜: 10.1 GB
- è®­ç»ƒæ˜¾å­˜: 52.0 GB
- Phase 3 é€Ÿåº¦: ~10000 tokens/s
- æ€»æ—¶é—´: ~40 å°æ—¶
- æˆæœ¬: ~Â¥355
```

### RTX 6000 Ada (é¢„ä¼°)

```
é…ç½®:
- small æ¨¡å‹ (17M)
- batch_size=8, seq_len=8192
- 529K æ ·æœ¬, 3 epochs

é¢„ä¼°:
- ç¼–è¯‘æ˜¾å­˜: ~10 GB
- è®­ç»ƒæ˜¾å­˜: ~65-70 GB
- Phase 3 é€Ÿåº¦: ~9000-10000 tokens/s
- æ€»æ—¶é—´: ~50-52 å°æ—¶
- æˆæœ¬: ~Â¥260
```

## æ•°æ®é¢„å¤„ç†

å¦‚æœéœ€è¦ä»å¤´é¢„å¤„ç†æ•°æ®ï¼š

```bash
# 1. ç”Ÿæˆ MIDI æ–‡ä»¶åˆ—è¡¨
find /path/to/midi/ -name "*.mid" -o -name "*.midi" > midi_files.txt

# 2. é¢„å¤„ç†
python preprocess_data.py \
    --input midi_files.txt \
    --output processed_data/ \
    --workers 10 \
    --shard-size 10000
```

## è®¸å¯è¯

MIT License

---

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹é¡¹ç›®ä¸»é¡µ: [../README.md](../README.md)
